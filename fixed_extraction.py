import re
import csv
import os
from datetime import datetime
from google.oauth2 import service_account
from google.cloud import vision
import glob

# è¡“å‰è¨ºæ–­ã®äº‹å‰å®šç¾©ãƒªã‚¹ãƒˆï¼ˆè¿½åŠ å¯èƒ½ï¼‰
PREDEFINED_DIAGNOSES = {
    'ç™½å†…éšœ': ['ç™½å†…éšœ', 'cataract', 'CATARACT'],
    'é»„æ–‘å‰è†œ': ['é»„æ–‘å‰è†œ', 'epiretinal membrane', 'ERM', 'é»„æ–‘éƒ¨å‰è†œ'],
    'ç¡å­ä½“å‡ºè¡€': ['ç¡å­ä½“å‡ºè¡€', 'vitreous hemorrhage', 'vitreous bleeding', 'ç¡å­ä½“å‡ºè¡€'],
    'ç¶²è†œå‰¥é›¢': ['ç¶²è†œå‰¥é›¢', 'retinal detachment', 'RD', 'ç¶²è†œã¯ãé›¢'],
    'ç·‘å†…éšœ': ['ç·‘å†…éšœ', 'glaucoma', 'GLAUCOMA'],
    'ç³–å°¿ç—…ç¶²è†œç—‡': ['ç³–å°¿ç—…ç¶²è†œç—‡', 'diabetic retinopathy', 'DR'],
    'åŠ é½¢é»„æ–‘å¤‰æ€§': ['åŠ é½¢é»„æ–‘å¤‰æ€§', 'age-related macular degeneration', 'AMD'],
    'ç¶²è†œé™è„ˆé–‰å¡ç—‡': ['ç¶²è†œé™è„ˆé–‰å¡ç—‡', 'retinal vein occlusion', 'RVO'],
    'ç¶²è†œå‹•è„ˆé–‰å¡ç—‡': ['ç¶²è†œå‹•è„ˆé–‰å¡ç—‡', 'retinal artery occlusion', 'RAO'],
    'é»„æ–‘å††å­”': ['é»„æ–‘å††å­”', 'macular hole', 'MH'],
    'ä¸­å¿ƒæ€§æ¼¿æ¶²æ€§è„ˆçµ¡ç¶²è†œç—‡': ['ä¸­å¿ƒæ€§æ¼¿æ¶²æ€§è„ˆçµ¡ç¶²è†œç—‡', 'central serous chorioretinopathy', 'CSC'],
    'ã¶ã©ã†è†œç‚': ['ã¶ã©ã†è†œç‚', 'uveitis', 'UVEITIS'],
    'è§’è†œç–¾æ‚£': ['è§’è†œç–¾æ‚£', 'corneal disease', 'è§’è†œç—…å¤‰'],
    'è¦–ç¥çµŒç–¾æ‚£': ['è¦–ç¥çµŒç–¾æ‚£', 'optic nerve disease', 'è¦–ç¥çµŒç—‡'],
}

# è¡“å¼ã®äº‹å‰å®šç¾©ãƒªã‚¹ãƒˆï¼ˆè¿½åŠ å¯èƒ½ï¼‰
PREDEFINED_SURGERIES = {
    'æ°´æ™¶ä½“å†å»ºè¡“': ['æ°´æ™¶ä½“å†å»ºè¡“', 'cataract surgery', 'phacoemulsification', 'PEA', 'ç™½å†…éšœæ‰‹è¡“'],
    'ç¡å­ä½“æ‰‹è¡“': ['ç¡å­ä½“æ‰‹è¡“', 'vitrectomy', 'vitreous surgery', 'ç¡å­ä½“åˆ‡é™¤è¡“'],
    'ãƒˆãƒ©ãƒ™ã‚¯ãƒ¬ã‚¯ãƒˆãƒŸãƒ¼': ['ãƒˆãƒ©ãƒ™ã‚¯ãƒ¬ã‚¯ãƒˆãƒŸãƒ¼', 'trabeculectomy', 'TRAB'],
    'ã‚«ãƒ•ãƒ¼ã‚¯': ['ã‚«ãƒ•ãƒ¼ã‚¯', 'Kahook', 'KDB', 'Kahook Dual Blade'],
    'ãƒã‚¤ã‚¯ãƒ­ã‚·ãƒ£ãƒ³ãƒˆ': ['ãƒã‚¤ã‚¯ãƒ­ã‚·ãƒ£ãƒ³ãƒˆ', 'microshunt', 'MicroShunt', 'iStent', 'XEN'],
    'ãƒ¬ãƒ¼ã‚¶ãƒ¼å…‰å‡å›ºè¡“': ['ãƒ¬ãƒ¼ã‚¶ãƒ¼å…‰å‡å›ºè¡“', 'laser photocoagulation', 'PRP', 'ç¶²è†œå…‰å‡å›º'],
    'ãƒ¬ãƒ¼ã‚¶ãƒ¼è™¹å½©åˆ‡é–‹è¡“': ['ãƒ¬ãƒ¼ã‚¶ãƒ¼è™¹å½©åˆ‡é–‹è¡“', 'laser iridotomy', 'LI', 'YAGè™¹å½©åˆ‡é–‹'],
    'ãƒ¬ãƒ¼ã‚¶ãƒ¼ç·šç¶­æŸ±å¸¯å½¢æˆè¡“': ['ãƒ¬ãƒ¼ã‚¶ãƒ¼ç·šç¶­æŸ±å¸¯å½¢æˆè¡“', 'laser trabeculoplasty', 'SLT', 'ALT'],
    'è§’è†œç§»æ¤è¡“': ['è§’è†œç§»æ¤è¡“', 'corneal transplantation', 'PKP', 'DSAEK', 'DMEK'],
    'ç¶²è†œå…‰å‡å›ºè¡“': ['ç¶²è†œå…‰å‡å›ºè¡“', 'retinal photocoagulation', 'ç¶²è†œãƒ¬ãƒ¼ã‚¶ãƒ¼'],
    'ç¡å­ä½“æ³¨å°„': ['ç¡å­ä½“æ³¨å°„', 'intravitreal injection', 'IVI', 'æŠ—VEGFæ³¨å°„'],
    'ç¶²è†œå‰¥é›¢æ‰‹è¡“': ['ç¶²è†œå‰¥é›¢æ‰‹è¡“', 'retinal detachment surgery', 'ç¶²è†œå¾©ä½è¡“'],
    'é»„æ–‘å‰è†œé™¤å»è¡“': ['é»„æ–‘å‰è†œé™¤å»è¡“', 'epiretinal membrane removal', 'ERMé™¤å»'],
    'é»„æ–‘å††å­”æ‰‹è¡“': ['é»„æ–‘å††å­”æ‰‹è¡“', 'macular hole surgery', 'å††å­”é–‰é–è¡“'],
    'ç·‘å†…éšœæ‰‹è¡“': ['ç·‘å†…éšœæ‰‹è¡“', 'glaucoma surgery', 'ç·šç¶­æŸ±å¸¯åˆ‡é™¤è¡“'],
}

def add_diagnosis(category, keywords):
    """è¡“å‰è¨ºæ–­ã®ã‚«ãƒ†ã‚´ãƒªã¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è¿½åŠ """
    global PREDEFINED_DIAGNOSES
    if category not in PREDEFINED_DIAGNOSES:
        PREDEFINED_DIAGNOSES[category] = []
    PREDEFINED_DIAGNOSES[category].extend(keywords)
    print(f"âœ… è¡“å‰è¨ºæ–­è¿½åŠ : {category} - {keywords}")

def add_surgery(category, keywords):
    """è¡“å¼ã®ã‚«ãƒ†ã‚´ãƒªã¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è¿½åŠ """
    global PREDEFINED_SURGERIES
    if category not in PREDEFINED_SURGERIES:
        PREDEFINED_SURGERIES[category] = []
    PREDEFINED_SURGERIES[category].extend(keywords)
    print(f"âœ… è¡“å¼è¿½åŠ : {category} - {keywords}")

def create_vision_client():
    """ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆèªè¨¼ã§Vision APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆ"""
    try:
        # ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚­ãƒ¼æƒ…å ±
        service_account_info = {
            "type": "service_account",
            "project_id": "sakuraganka-ai-466022",
            "private_key_id": "41f9b169fee0c14e34015b1c5f2a54d27c009108",
            "private_key": "-----BEGIN PRIVATE KEY-----\nMIIEvAIBADANBgkqhkiG9w0BAQEFAASCBKYwggSiAgEAAoIBAQDQth9HXQGtmsjx\ntkDvTFbQq8AJpbHmA96SBkZPw1NfCeQXC7Q+dVOhegxutOg2rMS4imNhkkHpA6at\nNVe6RyYVWBNU6Ahowv8TW0xCt7RMLyzQ0FW23x11tX7eHPE5HQYDYGn/Zzs9VPdu\nJERBxlViwROZyEJT4tqVk9798Qlgy0hiYNmdf/y8/0Qx5EW2CoaGEm1uOhakjLuE\noenBIIOgr/z5nCfOytDD0My4dF0YL5U7KsQvUMGU9Cw17jUD8J1blJN6MAmnd3Ay\nkpcUhUh8vKlPCg57WAmdiRngSc7KR+oc0HTfqvOqgdL+GbmjmQIJrhcI9UWOuIl6\ngmSxkz3hAgMBAAECgf80wZ6SPDpfhf1SsDGRWQSzx/mhM0e942Tsukr+IeF+lwgy\nekLmQ3EqdEpR53YphEyGdCPgwqUcVZNMJ6I0bTNv026Iyyul1kYqijOACCV2y55Z\nRQzRXA3MMzYokJQYQMtYiGuGHQSSTRcV1HNRi1C5My/VZWRsLjXBpva9szak6Oot\ntdOnN/7A6Xj+hFVjp6a9Bdor9tGaydOt1oEl+NT2Iy+FPirc8Tdc6i2ffgmXbtug\ney3gZwik/BkfZnpIbEt0rZMylEr2Tk48AxrjIs3RdeXN4Q3N6DK8Qr7LmMc4GNZT\nfgkbw3GLmQBz8ZczPcDlSIGNk+wZSlZ6nFkcWLsCgYEA93XbUXezwrHwHLs+I4yN\n2cr624MnGI2iDatcL5PolYrML2bs30kZOikLU4I+q3vl7geotE3wd+5fVjBC+Hm6\n4LFxVx2TkvmV3MNW/4UUQEQ1K8vQS76SvgInCULpPt/lIctaeyqZW7yb4Nsa9irv\nfOpHkUigrZrUZBLAOCi+WoMCgYEA1+nx4vLq30RfkTDwNeWnnVQraIvq8uLbBYgl\nP5e1j9G3fvjI4Ql546+YVhUNtNF/x9yxyBarFEQga9L5i3UNc9o9JBOKzac+hpLv\nFWhiiI7u2QRDy7GT1ZxGPZNiCo20E18LZV8/F0fJwTbjG2S20iQl8sZZYEkK6qoR\n+tT/KMsCgYEA3EgSsqOu5lqFVt4rQ3Pj9gMlafCHBelWX3qyNjwhJ7WFa5DgvScS\nCN7ukSj45qgFFu3EdLSIogoU3eFaTFv4SfpK3XSboJMCn6FXuV/alhbhihoFUtfT\nQschvrHMdcbS7lFaOxfBqpLr467HgmjYBUd7681OExwngunaKGPEh0cCgYEAxrMt\nP8Z+D+pEaMG4zmEC1+7V4+if19ad6YFZhiR/mlNNozQg6bhmy/qVHuNRMc564dtg\nYNs7pfLsQ05tCMI4Fx4IlmLFomz/RamDDRh7VWD0vhMGsTZC7ppaqeAwobW2uv0E\n5823qh0Otxlj95nABbPumHWhWtLdkQfidAwApfECgYBaVlRxkYao2IOo/DzaWCQw\nC9gMfXpB+qtYLXcbmPt2nVEPPXXS0XTzO0OhFd/F4e+knfDE4xTWwh15DrAZP/Qn\nYdyR1FwEDbe6ZHG8RbHa2wuxaiXQ5tLTyfae7IlaQpcKGGEUhkBXUgS9jbMc0HPi\nlDEGPIq2ToOq1uCd1894yA==\n-----END PRIVATE KEY-----\n",
            "client_email": "sakura-aidata@sakuraganka-ai-466022.iam.gserviceaccount.com",
            "client_id": "112249976566455945098",
            "auth_uri": "https://accounts.google.com/o/oauth2/auth",
            "token_uri": "https://oauth2.googleapis.com/token",
            "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",
            "client_x509_cert_url": "https://www.googleapis.com/robot/v1/metadata/x509/sakura-aidata%40sakuraganka-ai-466022.iam.gserviceaccount.com",
            "universe_domain": "googleapis.com"
        }
        
        credentials = service_account.Credentials.from_service_account_info(service_account_info)
        client = vision.ImageAnnotatorClient(credentials=credentials)
        return client
    except Exception as e:
        print(f"èªè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def google_vision_ocr(image_path, client):
    """Google Vision APIã§OCRå®Ÿè¡Œ"""
    try:
        with open(image_path, 'rb') as f:
            content = f.read()
        
        image = vision.Image(content=content)
        response = client.text_detection(image=image)
        texts = response.text_annotations
        
        if texts:
            return texts[0].description
        else:
            return ""
            
    except Exception as e:
        print(f"OCRã‚¨ãƒ©ãƒ¼ {image_path}: {e}")
        return ""

def reconstruct_vision_line(lines, start_index):
    """V.d./V.s.ã®è¡Œã‚’å†æ§‹ç¯‰"""
    
    # V.d.= ã¾ãŸã¯ V.s.= ã‹ã‚‰å§‹ã¾ã‚‹è¡Œã‚’è¦‹ã¤ã‘ãŸã‚‰
    # ãã®å¾Œã®æ•°å€¤ã‚„æ‹¬å¼§ã‚’å«ã‚€è¦ç´ ã‚’é›†ã‚ã‚‹
    
    reconstructed = lines[start_index]  # V.d.= ã¾ãŸã¯ V.s.=
    
    # æ¬¡ã®3-4è¡Œã‚’ç¢ºèª
    for j in range(1, 5):
        if start_index + j < len(lines):
            next_line = lines[start_index + j].strip()
            
            # æ•°å€¤ã€IOLã€æ‹¬å¼§ã€n.c.ãªã©ãŒå«ã¾ã‚Œã¦ã„ã‚Œã°è¿½åŠ 
            if any(x in next_line for x in ['0.', '1.', 'IOL', '(', 'n.c', 'Ã—', 'x']):
                reconstructed += ' ' + next_line
            # æ¬¡ã®V.d./V.s.ãŒæ¥ãŸã‚‰çµ‚äº†
            elif 'V.d.' in next_line or 'V.s.' in next_line:
                break
    
    return reconstructed

def fix_corrected_vision(value):
    """çŸ¯æ­£è¦–åŠ›ã®èª¤èªè­˜ã‚’ä¿®æ­£"""
    if value == '12':
        return '1.2'
    elif value == '10':
        return '1.0'
    elif value == '15':
        return '1.5'
    elif value == '20':
        return '2.0'
    # 1.265ã®ã‚ˆã†ãªå€¤ã¯å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
    try:
        v = float(value)
        if v > 2.5:  # è¦–åŠ›ã§2.5ä»¥ä¸Šã¯ã‚ã‚Šãˆãªã„
            # å°æ•°ç‚¹ã‚’è¿½åŠ 
            if str(value).startswith('12'):
                return '1.2'
    except:
        pass
    return value

def extract_handwritten_iop_patterns_fixed(text):
    """æ‰‹æ›¸ãçœ¼åœ§æŠ½å‡ºï¼ˆDATEèª¤èªè­˜ã‚’é˜²ãï¼‰"""
    
    lines = text.split('\n')
    result = {'å³çœ¼åœ§': '', 'å·¦çœ¼åœ§': '', 'çœ¼åœ§ãƒ¡ãƒ¢': ''}
    
    for line in lines:
        # DATEã®è¡Œã¯ã‚¹ã‚­ãƒƒãƒ—
        if 'DATE' in line.upper() or '2025/' in line or '2024/' in line:
            continue
            
        # AT/IOPãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¢ã™
        if any(marker in line.upper() for marker in ['AT', 'IOP', 'ï¼¡ï¼´', 'ï¼©ï¼¯ï¼°']):
            # DATEè¡Œã§ãªã„ã“ã¨ã‚’å†ç¢ºèª
            if 'DATE' not in line.upper():
                print(f"çœ¼åœ§è¡Œå€™è£œï¼ˆDATEé™¤å¤–å¾Œï¼‰: {line}")
                
                # æ•°å­—ã‚’æ¢ã™ï¼ˆâ—¯â—¯.â—¯å½¢å¼ï¼‰
                numbers = re.findall(r'\b(\d{1,2}\.\d)\b', line)
                valid_iop = [n for n in numbers if 0 <= float(n) <= 80]
                
                if len(valid_iop) >= 2:
                    result['å³çœ¼åœ§'] = valid_iop[0]
                    result['å·¦çœ¼åœ§'] = valid_iop[1]
                    return result
                    
                # ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆâ—¯â—¯.â—¯å½¢å¼ï¼‰
                slash = re.search(r'(\d{1,2}\.\d)\s*[/ï¼]\s*(\d{1,2}\.\d)', line)
                if slash:
                    v1, v2 = float(slash.group(1)), float(slash.group(2))
                    if 0 <= v1 <= 80 and 0 <= v2 <= 80:
                        result['å³çœ¼åœ§'] = str(v1)
                        result['å·¦çœ¼åœ§'] = str(v2)
                        return result
    
    return result

def debug_nct_structure(text):
    """NCTæ§‹é€ ã‚’è©³ç´°ã«åˆ†æ"""
    
    lines = text.split('\n')
    
    for i, line in enumerate(lines):
        if 'IOP' in line and 'mmHg' in line:
            print(f"\n=== NCTçœ¼åœ§æ§‹é€  è¡Œ{i} ===")
            print(f"ãƒ˜ãƒƒãƒ€ãƒ¼: {line}")
            
            # æ¬¡ã®10è¡Œã‚’è¡¨ç¤º
            for offset in range(1, 11):
                if i + offset < len(lines):
                    next_line = lines[i + offset]
                    print(f"  +{offset}è¡Œç›®: {next_line}")
                    
                    # ç‰¹å®šã®è¡Œã«ãƒãƒ¼ã‚¯
                    if offset == 1:
                        print("      ^ [R][L]è¡Œ")
                    elif offset in [2, 3, 4]:
                        print(f"      ^ {offset-1}å›ç›®æ¸¬å®š")
                    elif offset == 5:
                        print("      ^ â˜…Avgè¡Œï¼ˆã“ã“ã‚’å–å¾—ï¼‰")
            
            break

def extract_nct_by_position_improved(text):
    """NCTçœ¼åœ§ã‚’æ”¹è‰¯ç‰ˆã§å–å¾—ï¼ˆå¹³å‡å€¤ã®ã¿ã€å°æ•°ç‚¹ä»˜ãã‚’å„ªå…ˆï¼‰"""
    
    import re
    
    lines = text.split('\n')
    result = {
        'NCTå³': '',
        'NCTå·¦': '',
        'çœ¼åœ§å‚™è€ƒ': ''
    }
    
    # IOPãƒ˜ãƒƒãƒ€ãƒ¼ã‚’æ¢ã™
    iop_start = -1
    for i, line in enumerate(lines):
        if 'IOP' in line and 'mmHg' in line:
            iop_start = i
            print(f"NCTçœ¼åœ§ãƒ˜ãƒƒãƒ€ãƒ¼ç™ºè¦‹ è¡Œ{i}: {line}")
            break
    
    if iop_start == -1:
        return result
    
    # å¹³å‡å€¤ï¼ˆAvgï¼‰è¡Œã‚’æ¢ã™
    avg_line = -1
    for offset in range(1, 20):  # 20è¡Œä»¥å†…ã§æ¤œç´¢ï¼ˆç¯„å›²æ‹¡å¤§ï¼‰
        if iop_start + offset < len(lines):
            line = lines[iop_start + offset]
            if 'Avg' in line or 'AVG' in line or 'avg' in line or 'Aug' in line:
                avg_line = iop_start + offset
                print(f"  âœ… Avgè¡Œç™ºè¦‹ è¡Œ{avg_line}: {line}")
                break
    
    if avg_line == -1:
        print(f"  âŒ Avgè¡ŒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return result
    
    # Avgè¡Œã‹ã‚‰çœ¼åœ§å€¤ã‚’æŠ½å‡ºï¼ˆAvgè¡Œã«ä¸¦ã‚“ã§ã„ã‚‹ï¼‰
    avg_line_content = lines[avg_line]
    print(f"  ğŸ“Š Avgè¡Œå†…å®¹: {avg_line_content}")
    
    # Avgè¡Œã‹ã‚‰â—¯â—¯.â—¯å½¢å¼ã®çœ¼åœ§å€¤ã‚’ç›´æ¥æ¤œç´¢
    iop_values = re.findall(r'\b(\d{1,2}\.\d)\b', avg_line_content)
    print(f"  ğŸ” Avgè¡Œã®çœ¼åœ§å€¤å€™è£œ: {iop_values}")
    
    # çœ¼åœ§ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆ0-80 mmHgã€â—¯â—¯.â—¯å½¢å¼ï¼‰
    valid_iop_values = []
    for num_str in iop_values:
        try:
            val = float(num_str)
            if 0 <= val <= 80:
                valid_iop_values.append(num_str)
                print(f"    âœ… æœ‰åŠ¹ãªçœ¼åœ§å€¤: {num_str}")
            else:
                print(f"    âš ï¸ çœ¼åœ§ç¯„å›²å¤–: {num_str} (0-80)")
        except:
            continue
    
    # 2ã¤ã®æœ‰åŠ¹ãªå€¤ãŒã‚ã‚Œã°æˆåŠŸ
    if len(valid_iop_values) >= 2:
        result['NCTå³'] = valid_iop_values[0]
        result['NCTå·¦'] = valid_iop_values[1]
        result['çœ¼åœ§å‚™è€ƒ'] = f'NCTå¹³å‡å€¤ï¼ˆAvgè¡Œï¼‰'
        print(f"  âœ… NCTå¹³å‡å€¤å–å¾—æˆåŠŸ: R={valid_iop_values[0]}, L={valid_iop_values[1]}")
        return result
    
    # 1ã¤ã®å€¤ã—ã‹ãªã„å ´åˆï¼ˆ1å›è¨ˆæ¸¬ã®å¯èƒ½æ€§ï¼‰
    if len(valid_iop_values) == 1:
        result['NCTå³'] = valid_iop_values[0]
        result['NCTå·¦'] = ''  # å·¦çœ¼ã¯æ¸¬å®šã•ã‚Œã¦ã„ãªã„å¯èƒ½æ€§
        result['çœ¼åœ§å‚™è€ƒ'] = f'NCTå¹³å‡å€¤ï¼ˆ1å›è¨ˆæ¸¬ã€å³çœ¼ã®ã¿ï¼‰'
        print(f"  âš ï¸ NCTå¹³å‡å€¤ï¼ˆ1å›è¨ˆæ¸¬ï¼‰: R={valid_iop_values[0]}, L=æœªæ¸¬å®š")
        return result
    
    # æ¬¡ã®è¡Œã‚‚ãƒã‚§ãƒƒã‚¯ï¼ˆ2ã¤ç›®ã®å€¤ã‚’æ¢ã™ï¼‰
    if len(valid_iop_values) == 1:
        print(f"  âš ï¸ çœ¼åœ§å€¤ãŒ1ã¤ã—ã‹ã‚ã‚Šã¾ã›ã‚“ã€‚æ¬¡ã®è¡Œã‚’ãƒã‚§ãƒƒã‚¯...")
        
        # æ¬¡ã®10è¡Œã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆ1å›è¨ˆæ¸¬å¯¾å¿œï¼‰
        for offset in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]:
            check_line = avg_line + offset
            if check_line < len(lines):
                check_line_content = lines[check_line]
                print(f"  ğŸ“Š +{offset}è¡Œç›®å†…å®¹: {check_line_content}")
                
                # æ¬¡ã®è¡Œã‹ã‚‰ã‚‚â—¯â—¯.â—¯å½¢å¼ã®çœ¼åœ§å€¤ã‚’æ¤œç´¢
                check_iop_values = re.findall(r'\b(\d{1,2}\.\d)\b', check_line_content)
                print(f"  ğŸ” +{offset}è¡Œç›®ã®çœ¼åœ§å€¤å€™è£œ: {check_iop_values}")
                
                for num_str in check_iop_values:
                    try:
                        val = float(num_str)
                        if 0 <= val <= 80:
                            valid_iop_values.append(num_str)
                            print(f"    âœ… +{offset}è¡Œç›®ã®æœ‰åŠ¹ãªçœ¼åœ§å€¤: {num_str}")
                        else:
                            print(f"    âš ï¸ +{offset}è¡Œç›®ã®çœ¼åœ§ç¯„å›²å¤–: {num_str} (0-80)")
                    except:
                        continue
                
                if len(valid_iop_values) >= 2:
                    result['NCTå³'] = valid_iop_values[0]
                    result['NCTå·¦'] = valid_iop_values[1]
                    result['çœ¼åœ§å‚™è€ƒ'] = f'NCTå¹³å‡å€¤ï¼ˆ1å›è¨ˆæ¸¬ã€Avgè¡Œ+{offset}è¡Œç›®ï¼‰'
                    print(f"  âœ… NCTå¹³å‡å€¤å–å¾—æˆåŠŸ: R={valid_iop_values[0]}, L={valid_iop_values[1]}")
                    return result
    
    print(f"  âŒ æœ‰åŠ¹ãªçœ¼åœ§å€¤ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
    return result

def debug_nct_detection(text):
    """NCTçœ¼åœ§ãŒæ¤œå‡ºã§ããªã„åŸå› ã‚’èª¿æŸ»"""
    
    lines = text.split('\n')
    
    print("=== NCTçœ¼åœ§æ¤œå‡ºãƒ‡ãƒãƒƒã‚° ===")
    
    # IOPã‚’å«ã‚€è¡Œã‚’å…¨ã¦è¡¨ç¤º
    iop_found = False
    for i, line in enumerate(lines):
        if 'IOP' in line.upper():
            iop_found = True
            print(f"ğŸ” IOPè¡Œç™ºè¦‹ {i}: {line}")
            # æ¬¡ã®10è¡Œã‚’è¡¨ç¤º
            for j in range(i+1, min(i+10, len(lines))):
                line_content = lines[j].strip()
                if line_content:
                    marker = "  "
                    if 'Avg' in line_content or 'AVG' in line_content:
                        marker = "â˜… "
                    elif '*' in line_content:
                        marker = "* "
                    elif any(char.isdigit() for char in line_content):
                        marker = "ğŸ”¢"
                    print(f"  {marker}{j}: {line_content}")
    
    if not iop_found:
        print("âŒ IOPè¡ŒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
    
    # æ•°å€¤ãƒ‘ã‚¿ãƒ¼ãƒ³ã§çœ¼åœ§å€™è£œã‚’æ¢ã™
    print(f"\n=== çœ¼åœ§å€™è£œæ¤œç´¢ ===")
    for i, line in enumerate(lines):
        import re
        # 0-80ã®ç¯„å›²ã®æ•°å€¤ãƒšã‚¢ã‚’æ¢ã™ï¼ˆæ‰‹æ›¸ãçœ¼åœ§ï¼‰
        numbers = re.findall(r'\b(\d{1,2})\b', line)
        valid = [n for n in numbers if 0 <= int(n) <= 80]
        if len(valid) >= 2:
            print(f"ğŸ”¢ çœ¼åœ§å€™è£œï¼ˆæ•°å€¤ï¼‰è¡Œ{i}: {line}")
            print(f"   æ¤œå‡ºæ•°å€¤: {valid}")
        elif len(valid) == 1:
            print(f"ğŸ”¢ å˜ä¸€æ•°å€¤è¡Œ{i}: {line} (å€¤: {valid[0]})")
    
    # å°æ•°ç‚¹ã‚’å«ã‚€æ•°å€¤ã‚’æ¢ã™ï¼ˆNCTå¹³å‡å€¤ã®å¯èƒ½æ€§ï¼‰
    print(f"\n=== å°æ•°ç‚¹ä»˜ãæ•°å€¤æ¤œç´¢ï¼ˆNCTå¹³å‡å€¤å€™è£œï¼‰ ===")
    for i, line in enumerate(lines):
        import re
        decimal_numbers = re.findall(r'\b(\d+\.\d+)\b', line)
        if decimal_numbers:
            print(f"ğŸ“Š å°æ•°ç‚¹ä»˜ãè¡Œ{i}: {line}")
            print(f"   æ¤œå‡ºå€¤: {decimal_numbers}")
    
    print(f"\n=== ãƒ‡ãƒãƒƒã‚°å®Œäº† ===")

def fix_s_five_confusion(text):
    """Sï¼ˆçƒé¢åº¦æ•°ï¼‰ã¨5ã®èª¤èªè­˜ã‚’ä¿®æ­£"""
    
    import re
    
    # ã‚ˆãã‚ã‚‹èª¤èªè­˜ãƒ‘ã‚¿ãƒ¼ãƒ³
    # 12x5 â†’ 1.2Ã—S
    # 1,285 â†’ 1.2Ã—S
    # 12Ã—5 â†’ 1.2Ã—S
    
    # æ‹¬å¼§å†…ã®å‡¦ç†
    def fix_in_brackets(match):
        content = match.group(0)
        # 12x5 â†’ 1.2Ã—S
        content = re.sub(r'12x5', '1.2Ã—S', content)
        content = re.sub(r'12Ã—5', '1.2Ã—S', content)
        content = re.sub(r'1,285', '1.2Ã—S', content)
        # 10x5 â†’ 1.0Ã—S
        content = re.sub(r'10x5', '1.0Ã—S', content)
        content = re.sub(r'10Ã—5', '1.0Ã—S', content)
        return content
    
    # æ‹¬å¼§å†…ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä¿®æ­£
    text = re.sub(r'\([^)]+\)', fix_in_brackets, text)
    
    # V.5. â†’ V.s.ã®ä¿®æ­£
    text = text.replace('V.5.', 'V.s.')
    text = text.replace('V.S.', 'V.s.')
    
    return text

def process_two_tier_vision_data(text):
    """2æ®µæ§‹é€ ï¼ˆå‰å›ãƒ»ä»Šå›ï¼‰ã®è¦–åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†"""
    
    lines = text.split('\n')
    result = {
        'å‰å›_å³è£¸çœ¼': '',
        'å‰å›_å³çŸ¯æ­£': '',
        'å‰å›_å·¦è£¸çœ¼': '',
        'å‰å›_å·¦çŸ¯æ­£': '',
        'ä»Šå›_å³è£¸çœ¼': '',
        'ä»Šå›_å³çŸ¯æ­£': '',
        'ä»Šå›_å·¦è£¸çœ¼': '',
        'ä»Šå›_å·¦çŸ¯æ­£': '',
        'ä»Šå›_S': '',
        'ä»Šå›_C': '',
        'ä»Šå›_A': ''
    }
    
    # 2æ®µæ§‹é€ ã‚’æ¤œå‡º
    upper_section = []
    lower_section = []
    current_section = upper_section
    
    for line in lines:
        # åŒºåˆ‡ã‚Šç·šã‚„æ˜ç¢ºãªåŒºåˆ‡ã‚Šã‚’æ¢ã™
        if any(marker in line for marker in ['---', '===', 'å‰å›', 'ä»Šå›', '2025', '2024']):
            if len(upper_section) > 0:
                current_section = lower_section
        current_section.append(line)
    
    # ä¸Šæ®µï¼ˆå‰å›ï¼‰ã®å‡¦ç†
    if upper_section:
        upper_text = '\n'.join(upper_section)
        upper_data = extract_vision_data_fixed(upper_text)
        result['å‰å›_å³è£¸çœ¼'] = upper_data['å³è£¸çœ¼']
        result['å‰å›_å³çŸ¯æ­£'] = upper_data['å³çŸ¯æ­£']
        result['å‰å›_å·¦è£¸çœ¼'] = upper_data['å·¦è£¸çœ¼']
        result['å‰å›_å·¦çŸ¯æ­£'] = upper_data['å·¦çŸ¯æ­£']
    
    # ä¸‹æ®µï¼ˆä»Šå›ï¼‰ã®å‡¦ç†ï¼ˆå„ªå…ˆï¼‰
    if lower_section:
        lower_text = '\n'.join(lower_section)
        lower_data = extract_vision_data_fixed(lower_text)
        result['ä»Šå›_å³è£¸çœ¼'] = lower_data['å³è£¸çœ¼']
        result['ä»Šå›_å³çŸ¯æ­£'] = lower_data['å³çŸ¯æ­£']
        result['ä»Šå›_å·¦è£¸çœ¼'] = lower_data['å·¦è£¸çœ¼']
        result['ä»Šå›_å·¦çŸ¯æ­£'] = lower_data['å·¦çŸ¯æ­£']
        
        # åº¦æ•°æƒ…å ±ã‚‚æŠ½å‡º
        degree_data = extract_degree_data(lower_text)
        result['ä»Šå›_S'] = degree_data['S']
        result['ä»Šå›_C'] = degree_data['C']
        result['ä»Šå›_A'] = degree_data['A']
    
    return result

def extract_degree_data(text):
    """åº¦æ•°æƒ…å ±ï¼ˆSã€Cã€Aï¼‰ã‚’æŠ½å‡º"""
    
    import re
    
    result = {'S': '', 'C': '', 'A': ''}
    
    # Sï¼ˆçƒé¢åº¦æ•°ï¼‰ã‚’æ¢ã™
    s_patterns = [
        r'S[:\s]*([+-]?\d+\.?\d*)',
        r'çƒé¢[:\s]*([+-]?\d+\.?\d*)',
        r'([+-]?\d+\.?\d*)\s*Ã—\s*S'
    ]
    
    for pattern in s_patterns:
        match = re.search(pattern, text)
        if match:
            result['S'] = match.group(1)
            break
    
    # Cï¼ˆå††æŸ±åº¦æ•°ï¼‰ã‚’æ¢ã™
    c_patterns = [
        r'C[:\s]*([+-]?\d+\.?\d*)',
        r'å††æŸ±[:\s]*([+-]?\d+\.?\d*)',
        r'([+-]?\d+\.?\d*)\s*Ã—\s*C'
    ]
    
    for pattern in c_patterns:
        match = re.search(pattern, text)
        if match:
            result['C'] = match.group(1)
            break
    
    # Aï¼ˆè»¸ï¼‰ã‚’æ¢ã™
    a_patterns = [
        r'A[:\s]*(\d+)',
        r'è»¸[:\s]*(\d+)',
        r'(\d+)\s*åº¦'
    ]
    
    for pattern in a_patterns:
        match = re.search(pattern, text)
        if match:
            result['A'] = match.group(1)
            break
    
    return result

def extract_vision_data_fixed(text):
    """æ”¹è¡Œã‚’è€ƒæ…®ã—ãŸè¦–åŠ›ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºï¼ˆæœ€çµ‚ç‰ˆãƒ»TOLå¯¾å¿œï¼‰"""
    
    import re
    
    # Sã¨5ã®èª¤èªè­˜ã‚’ä¿®æ­£
    text = fix_s_five_confusion(text)
    
    # æ”¹è¡Œã‚’ä¸€æ—¦é™¤å»ã—ã¦é€£ç¶šã—ãŸãƒ†ã‚­ã‚¹ãƒˆã«ã™ã‚‹
    lines = text.split('\n')
    
    result = {
        'å³è£¸çœ¼': '',
        'å³çŸ¯æ­£': '',
        'å·¦è£¸çœ¼': '',
        'å·¦çŸ¯æ­£': '',
        'å³TOL': '',  # çœ¼å†…ãƒ¬ãƒ³ã‚ºæƒ…å ±è¿½åŠ 
        'å·¦TOL': '',  # çœ¼å†…ãƒ¬ãƒ³ã‚ºæƒ…å ±è¿½åŠ 
        'å³çœ¼åœ§': '',
        'å·¦çœ¼åœ§': ''
    }
    
    # V.d.ã‚’æ¢ã—ã¦ã€æ¬¡ã®æ•°è¡Œã‚‚å«ã‚ã¦å‡¦ç†
    for i, line in enumerate(lines):
        if 'V.d.' in line or 'Vd' in line:
            # ç¾åœ¨ã®è¡Œã¨æ¬¡ã®3è¡Œã‚’çµåˆ
            combined = ' '.join(lines[i:min(i+4, len(lines))])
            print(f"V.d.çµåˆãƒ†ã‚­ã‚¹ãƒˆ: {combined}")
            
            # è£¸çœ¼è¦–åŠ›ã‚’æ¢ã™ï¼ˆ0.01, 0.1ãªã©ï¼‰
            naked = re.search(r'V\.?d\.?\s*=?\s*([\d\.]+)', combined)
            if naked:
                result['å³è£¸çœ¼'] = naked.group(1)
            
            # TOLï¼ˆçœ¼å†…ãƒ¬ãƒ³ã‚ºï¼‰ã‚’æ¢ã™
            tol_pattern = re.search(r'([\d\.]+)\s*[xXÃ—]\s*(?:TOL|IOL|FOL|EOL|1OL)', combined)
            if tol_pattern:
                result['å³TOL'] = tol_pattern.group(1)
                print(f"  âœ… å³çœ¼TOLç™ºè¦‹: {result['å³TOL']}")
            
            # çŸ¯æ­£è¦–åŠ›ã‚’æ¢ã™
            # ãƒ‘ã‚¿ãƒ¼ãƒ³1: (n.c.) ã¾ãŸã¯ (n.c)
            if 'n.c' in combined.lower():
                result['å³çŸ¯æ­£'] = 'n.c.'
            else:
                # ãƒ‘ã‚¿ãƒ¼ãƒ³2: (æ•°å€¤) ã¾ãŸã¯æ‹¬å¼§å†…ã®æœ€åˆã®æ•°å€¤
                corrected = re.search(r'\(([\d\.]+)', combined)
                if corrected:
                    result['å³çŸ¯æ­£'] = corrected.group(1)
                # ãƒ‘ã‚¿ãƒ¼ãƒ³3: Ã—IOL ã®å¾Œã®æ‹¬å¼§å†…
                iol_pattern = re.search(r'(?:TOL|IOL|FOL|EOL|1OL).*?\(([\d\.]+|n\.c\.?)', combined)
                if iol_pattern:
                    result['å³çŸ¯æ­£'] = iol_pattern.group(1)
        
        # V.s.ã‚‚åŒæ§˜ã«å‡¦ç†
        if 'V.s.' in line or 'Vs' in line:
            combined = ' '.join(lines[i:min(i+4, len(lines))])
            print(f"V.s.çµåˆãƒ†ã‚­ã‚¹ãƒˆ: {combined}")
            
            naked = re.search(r'V\.?s\.?\s*=?\s*([\d\.]+)', combined)
            if naked:
                result['å·¦è£¸çœ¼'] = naked.group(1)
            
            # TOLï¼ˆçœ¼å†…ãƒ¬ãƒ³ã‚ºï¼‰ã‚’æ¢ã™
            tol_pattern = re.search(r'([\d\.]+)\s*[xXÃ—]\s*(?:TOL|IOL|FOL|EOL|1OL)', combined)
            if tol_pattern:
                result['å·¦TOL'] = tol_pattern.group(1)
                print(f"  âœ… å·¦çœ¼TOLç™ºè¦‹: {result['å·¦TOL']}")
            
            if 'n.c' in combined.lower():
                result['å·¦çŸ¯æ­£'] = 'n.c.'
            else:
                corrected = re.search(r'\(([\d\.]+)', combined)
                if corrected:
                    result['å·¦çŸ¯æ­£'] = corrected.group(1)
    
    # çœ¼åœ§ï¼ˆã“ã‚Œã¯æ­£ç¢ºã«å–ã‚Œã¦ã„ã‚‹ï¼‰
    iop_found = False
    for i, line in enumerate(lines):
        if 'IOP' in line:
            iop_found = True
        if iop_found and '[R]' in line:
            nums = re.findall(r'(\d+)', line)
            if nums:
                result['å³çœ¼åœ§'] = nums[0]
        if iop_found and '[L]' in line:
            nums = re.findall(r'(\d+)', line)
            if nums:
                result['å·¦çœ¼åœ§'] = nums[0]
    
    return result

def extract_all_iop_types(text):
    """NCTã¨æ‰‹æ›¸ãçœ¼åœ§ã‚’ä¸¡æ–¹å–å¾—ï¼ˆä½ç½®ãƒ™ãƒ¼ã‚¹æ”¹è‰¯ç‰ˆï¼‰"""
    
    import re
    
    result = {
        'NCTå³': '',
        'NCTå·¦': '',
        'æ‰‹æ›¸ãå³': '',
        'æ‰‹æ›¸ãå·¦': '',
        'çœ¼åœ§å‚™è€ƒ': ''
    }
    
    lines = text.split('\n')
    
    # ========================================
    # 1. NCTçœ¼åœ§ã‚’ä½ç½®ãƒ™ãƒ¼ã‚¹ã§å–å¾—
    # ========================================
    nct_result = extract_nct_by_position_improved(text)
    
    if nct_result['NCTå³'] and nct_result['NCTå·¦']:
        result['NCTå³'] = nct_result['NCTå³']
        result['NCTå·¦'] = nct_result['NCTå·¦']
        result['çœ¼åœ§å‚™è€ƒ'] = nct_result['çœ¼åœ§å‚™è€ƒ']
        nct_found = True
    else:
        nct_found = False
    
    # ========================================
    # 2. æ‰‹æ›¸ãçœ¼åœ§ã‚‚æ¢ã™ï¼ˆNCTã®æœ‰ç„¡ã«é–¢ã‚ã‚‰ãšï¼‰
    # ========================================
    handwritten_result = extract_handwritten_iop_patterns_improved(text)
    
    if handwritten_result['å³çœ¼åœ§'] and handwritten_result['å·¦çœ¼åœ§']:
        result['æ‰‹æ›¸ãå³'] = handwritten_result['å³çœ¼åœ§']
        result['æ‰‹æ›¸ãå·¦'] = handwritten_result['å·¦çœ¼åœ§']
        handwritten_found = True
        print(f"  âœ… æ‰‹æ›¸ãçœ¼åœ§: R={handwritten_result['å³çœ¼åœ§']}, L={handwritten_result['å·¦çœ¼åœ§']}")
        print(f"     ãƒ¡ãƒ¢: {handwritten_result['çœ¼åœ§ãƒ¡ãƒ¢']}")
    else:
        handwritten_found = False
    
    # ========================================
    # 3. çµæœã®æ•´ç†
    # ========================================
    if nct_found and handwritten_found:
        result['çœ¼åœ§å‚™è€ƒ'] = 'NCT+æ‰‹æ›¸ãä¸¡æ–¹ã‚ã‚Š'
    elif nct_found:
        result['çœ¼åœ§å‚™è€ƒ'] = 'NCTã®ã¿'
    elif handwritten_found:
        result['çœ¼åœ§å‚™è€ƒ'] = 'æ‰‹æ›¸ãã®ã¿'
    else:
        result['çœ¼åœ§å‚™è€ƒ'] = 'æ¤œå‡ºå¤±æ•—'
    
    return result

def select_final_iop(iop_data):
    """æœ€çµ‚çš„ã«ä½¿ç”¨ã™ã‚‹çœ¼åœ§å€¤ã‚’æ±ºå®š"""
    
    result = {
        'çœ¼åœ§å³': '',
        'çœ¼åœ§å·¦': '',
        'ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿': ''
    }
    
    # å„ªå…ˆé †ä½ï¼š
    # 1. æ‰‹æ›¸ããŒã‚ã‚Œã°æ‰‹æ›¸ãï¼ˆåŒ»å¸«ãŒå†æ¸¬å®šã—ãŸå¯èƒ½æ€§ï¼‰
    # 2. æ‰‹æ›¸ããŒãªã‘ã‚Œã°NCT
    
    if iop_data['æ‰‹æ›¸ãå³'] and iop_data['æ‰‹æ›¸ãå·¦']:
        result['çœ¼åœ§å³'] = iop_data['æ‰‹æ›¸ãå³']
        result['çœ¼åœ§å·¦'] = iop_data['æ‰‹æ›¸ãå·¦']
        result['ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿'] = 'æ‰‹æ›¸ãå„ªå…ˆ'
    elif iop_data['NCTå³'] and iop_data['NCTå·¦']:
        result['çœ¼åœ§å³'] = iop_data['NCTå³']
        result['çœ¼åœ§å·¦'] = iop_data['NCTå·¦']
        result['ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿'] = 'NCT'
    
    return result



def process_image_final_comprehensive(ocr_text, filename=None):
    """æœ€çµ‚åŒ…æ‹¬çš„å‡¦ç†ï¼ˆè¦–åŠ›+çœ¼åœ§+ãƒ¬ãƒ•å€¤+æœ€çµ‚é¸æŠ+TOLå¯¾å¿œ+IOLã‚·ãƒ¼ãƒ«å¯¾å¿œ+æ¤œæŸ»ç”»åƒè­˜åˆ¥å¯¾å¿œ+å·¦å³åˆ¤å®šå¯¾å¿œ+è©³ç´°æ¤œæŸ»ãƒ‡ãƒ¼ã‚¿å¯¾å¿œï¼‰"""
    
    result = {
        'å³è£¸çœ¼': '',
        'å³çŸ¯æ­£': '',
        'å·¦è£¸çœ¼': '',
        'å·¦çŸ¯æ­£': '',
        'å³TOL': '',  # çœ¼å†…ãƒ¬ãƒ³ã‚ºæƒ…å ±è¿½åŠ 
        'å·¦TOL': '',  # çœ¼å†…ãƒ¬ãƒ³ã‚ºæƒ…å ±è¿½åŠ 
        'NCTå³': '',
        'NCTå·¦': '',
        'æ‰‹æ›¸ãå³': '',
        'æ‰‹æ›¸ãå·¦': '',
        'æœ€çµ‚çœ¼åœ§å³': '',
        'æœ€çµ‚çœ¼åœ§å·¦': '',
        'çœ¼åœ§å‚™è€ƒ': '',
        'ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿': '',
        'S': '',
        'C': '',
        'Ax': '',
        'æ‰‹è¡“æ—¥': '',
        'æ‚£è€…å': '',
        'è¡“å‰è¨ºæ–­': '',
        'è¡“å¼': '',
        'å¯¾è±¡çœ¼': '',
        'IOLåº¦æ•°_S': '',  # IOLã‚·ãƒ¼ãƒ«æƒ…å ±è¿½åŠ 
        'IOLåº¦æ•°_C': '',  # IOLã‚·ãƒ¼ãƒ«æƒ…å ±è¿½åŠ 
        'IOLåº¦æ•°_Ax': '',  # IOLã‚·ãƒ¼ãƒ«æƒ…å ±è¿½åŠ 
        'IOLè£½å“å': '',  # IOLã‚·ãƒ¼ãƒ«æƒ…å ±è¿½åŠ 
        'IOLãƒ¡ãƒ¼ã‚«ãƒ¼': '',  # IOLã‚·ãƒ¼ãƒ«æƒ…å ±è¿½åŠ 
        'IOLå‚™è€ƒ': '',  # IOLã‚·ãƒ¼ãƒ«æƒ…å ±è¿½åŠ 
        'æ¤œæŸ»ç¨®é¡': '',  # æ¤œæŸ»ç”»åƒè­˜åˆ¥è¿½åŠ 
        'æ¤œæŸ»è©³ç´°': '',  # æ¤œæŸ»ç”»åƒè­˜åˆ¥è¿½åŠ 
        'æ¤œæŸ»æ—¥': '',  # æ¤œæŸ»ç”»åƒè­˜åˆ¥è¿½åŠ 
        'æ¤œæŸ»å¯¾è±¡çœ¼': '',  # æ¤œæŸ»å¯¾è±¡çœ¼è¿½åŠ 
        'æ¤œæŸ»å‚™è€ƒ': ''  # æ¤œæŸ»ç”»åƒè­˜åˆ¥è¿½åŠ 
    }
    
    # è¦–åŠ›ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
    vision = extract_vision_data_fixed(ocr_text)
    
    # çŸ¯æ­£è¦–åŠ›ã®ä¿®æ­£
    if vision['å³çŸ¯æ­£']:
        vision['å³çŸ¯æ­£'] = fix_corrected_vision(vision['å³çŸ¯æ­£'])
    if vision['å·¦çŸ¯æ­£']:
        vision['å·¦çŸ¯æ­£'] = fix_corrected_vision(vision['å·¦çŸ¯æ­£'])
    
    # è¦–åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’çµæœã«è¿½åŠ 
    result['å³è£¸çœ¼'] = vision['å³è£¸çœ¼']
    result['å³çŸ¯æ­£'] = vision['å³çŸ¯æ­£']
    result['å·¦è£¸çœ¼'] = vision['å·¦è£¸çœ¼']
    result['å·¦çŸ¯æ­£'] = vision['å·¦çŸ¯æ­£']
    result['å³TOL'] = vision['å³TOL']  # TOLæƒ…å ±è¿½åŠ 
    result['å·¦TOL'] = vision['å·¦TOL']  # TOLæƒ…å ±è¿½åŠ 
    
    # åŒ…æ‹¬çš„ãªçœ¼åœ§ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
    iop_data = extract_all_iop_types(ocr_text)
    result['NCTå³'] = iop_data['NCTå³']
    result['NCTå·¦'] = iop_data['NCTå·¦']
    result['æ‰‹æ›¸ãå³'] = iop_data['æ‰‹æ›¸ãå³']
    result['æ‰‹æ›¸ãå·¦'] = iop_data['æ‰‹æ›¸ãå·¦']
    result['çœ¼åœ§å‚™è€ƒ'] = iop_data['çœ¼åœ§å‚™è€ƒ']
    
    # æœ€çµ‚çš„ãªçœ¼åœ§ãƒ‡ãƒ¼ã‚¿é¸æŠ
    final_iop = select_final_iop(iop_data)
    result['æœ€çµ‚çœ¼åœ§å³'] = final_iop['çœ¼åœ§å³']
    result['æœ€çµ‚çœ¼åœ§å·¦'] = final_iop['çœ¼åœ§å·¦']
    result['ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿'] = final_iop['ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿']
    
    # ãƒ¬ãƒ•å€¤ï¼ˆå±ˆæŠ˜å€¤ï¼‰æŠ½å‡º
    refraction_data = extract_refraction_data(ocr_text)
    result['S'] = refraction_data['S']
    result['C'] = refraction_data['C']
    result['Ax'] = refraction_data['Ax']
    
    # æ‰‹è¡“æƒ…å ±æŠ½å‡ºï¼ˆæ‰‹è¡“è¨˜éŒ²ã®å ´åˆï¼‰
    surgery_data = extract_surgery_data(ocr_text)
    result['æ‰‹è¡“æ—¥'] = surgery_data['æ‰‹è¡“æ—¥']
    result['æ‚£è€…å'] = surgery_data['æ‚£è€…å']
    result['è¡“å‰è¨ºæ–­'] = surgery_data['è¡“å‰è¨ºæ–­']
    result['è¡“å¼'] = surgery_data['è¡“å¼']
    result['å¯¾è±¡çœ¼'] = surgery_data['å¯¾è±¡çœ¼']
    
    # IOLã‚·ãƒ¼ãƒ«æƒ…å ±æŠ½å‡º
    iol_seal_data = extract_iol_seal_data(ocr_text)
    result['IOLåº¦æ•°_S'] = iol_seal_data['IOLåº¦æ•°_S']
    result['IOLåº¦æ•°_C'] = iol_seal_data['IOLåº¦æ•°_C']
    result['IOLåº¦æ•°_Ax'] = iol_seal_data['IOLåº¦æ•°_Ax']
    result['IOLè£½å“å'] = iol_seal_data['IOLè£½å“å']
    result['IOLãƒ¡ãƒ¼ã‚«ãƒ¼'] = iol_seal_data['IOLãƒ¡ãƒ¼ã‚«ãƒ¼']
    result['IOLå‚™è€ƒ'] = iol_seal_data['IOLå‚™è€ƒ']
    
    # æ¤œæŸ»ç”»åƒè­˜åˆ¥ï¼ˆOCRãƒ†ã‚­ã‚¹ãƒˆã®å†…å®¹ã®ã¿ã‹ã‚‰å·¦å³åˆ¤å®šï¼‰
    examination_data = identify_examination_type(ocr_text)
    result['æ¤œæŸ»ç¨®é¡'] = examination_data['æ¤œæŸ»ç¨®é¡']
    result['æ¤œæŸ»è©³ç´°'] = examination_data['æ¤œæŸ»è©³ç´°']
    result['æ¤œæŸ»æ—¥'] = examination_data['æ¤œæŸ»æ—¥']
    result['æ¤œæŸ»å¯¾è±¡çœ¼'] = examination_data['å¯¾è±¡çœ¼']  # OCRãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰åˆ¤å®šã—ãŸå¯¾è±¡çœ¼
    result['æ¤œæŸ»å‚™è€ƒ'] = examination_data['æ¤œæŸ»å‚™è€ƒ']
    
    # æ¤œæŸ»ç¨®é¡åˆ¥ã®è©³ç´°ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
    if examination_data['æ¤œæŸ»ç¨®é¡'] == 'OCT':
        oct_data = extract_oct_data(ocr_text, text_upper)
        result.update(oct_data)
    elif examination_data['æ¤œæŸ»ç¨®é¡'] == 'OCTA':
        octa_data = extract_octa_data(ocr_text, text_upper)
        result.update(octa_data)
    elif examination_data['æ¤œæŸ»ç¨®é¡'] in ['ãƒãƒ³ãƒ•ãƒªãƒ¼è¦–é‡', 'AIMOè¦–é‡']:
        visual_field_data = extract_visual_field_data(ocr_text, text_upper)
        result.update(visual_field_data)
    
    return result

def process_image_two_tier_comprehensive(ocr_text):
    """2æ®µæ§‹é€ å¯¾å¿œã®åŒ…æ‹¬çš„å‡¦ç†"""
    
    result = {
        'å‰å›_å³è£¸çœ¼': '',
        'å‰å›_å³çŸ¯æ­£': '',
        'å‰å›_å·¦è£¸çœ¼': '',
        'å‰å›_å·¦çŸ¯æ­£': '',
        'ä»Šå›_å³è£¸çœ¼': '',
        'ä»Šå›_å³çŸ¯æ­£': '',
        'ä»Šå›_å·¦è£¸çœ¼': '',
        'ä»Šå›_å·¦çŸ¯æ­£': '',
        'ä»Šå›_S': '',
        'ä»Šå›_C': '',
        'ä»Šå›_A': '',
        'NCTå³': '',
        'NCTå·¦': '',
        'æ‰‹æ›¸ãå³': '',
        'æ‰‹æ›¸ãå·¦': '',
        'æœ€çµ‚çœ¼åœ§å³': '',
        'æœ€çµ‚çœ¼åœ§å·¦': '',
        'çœ¼åœ§å‚™è€ƒ': '',
        'ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿': ''
    }
    
    # 2æ®µæ§‹é€ ã®è¦–åŠ›ãƒ‡ãƒ¼ã‚¿å‡¦ç†
    two_tier_data = process_two_tier_vision_data(ocr_text)
    
    # 2æ®µæ§‹é€ ãƒ‡ãƒ¼ã‚¿ã‚’çµæœã«è¿½åŠ 
    for key in two_tier_data:
        result[key] = two_tier_data[key]
    
    # çœ¼åœ§ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
    iop_data = extract_all_iop_types(ocr_text)
    result['NCTå³'] = iop_data['NCTå³']
    result['NCTå·¦'] = iop_data['NCTå·¦']
    result['æ‰‹æ›¸ãå³'] = iop_data['æ‰‹æ›¸ãå³']
    result['æ‰‹æ›¸ãå·¦'] = iop_data['æ‰‹æ›¸ãå·¦']
    result['çœ¼åœ§å‚™è€ƒ'] = iop_data['çœ¼åœ§å‚™è€ƒ']
    
    # æœ€çµ‚çš„ãªçœ¼åœ§ãƒ‡ãƒ¼ã‚¿é¸æŠ
    final_iop = select_final_iop(iop_data)
    result['æœ€çµ‚çœ¼åœ§å³'] = final_iop['çœ¼åœ§å³']
    result['æœ€çµ‚çœ¼åœ§å·¦'] = final_iop['çœ¼åœ§å·¦']
    result['ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿'] = final_iop['ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿']
    
    return result

def process_all_images_final_comprehensive():
    """æœ€çµ‚åŒ…æ‹¬çš„ã‚·ã‚¹ãƒ†ãƒ ã§å…¨ç”»åƒå‡¦ç†"""
    print("æœ€çµ‚åŒ…æ‹¬çš„åŒ»ç™‚OCRã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 50)
    
    # Vision APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆ
    client = create_vision_client()
    if not client:
        print("âŒ Vision APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
        return []
    
    # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
    image_folder = r"C:\Projects\medical-ocr\inbox"
    image_files = glob.glob(os.path.join(image_folder, "*.JPG"))
    image_files.extend(glob.glob(os.path.join(image_folder, "*.jpg")))
    
    print(f"å‡¦ç†å¯¾è±¡ç”»åƒæ•°: {len(image_files)}")
    
    results = []
    
    for i, img_file in enumerate(image_files, 1):
        filename = os.path.basename(img_file)
        print(f"\n[{i}/{len(image_files)}] å‡¦ç†ä¸­: {filename}")
        
        # Google Vision APIå®Ÿè¡Œ
        text = google_vision_ocr(img_file, client)
        
        if not text:
            print(f"  âŒ OCRå¤±æ•—")
            results.append({
                'filename': filename,
                'status': 'OCR_FAILED',
                'å³è£¸çœ¼': '',
                'å³çŸ¯æ­£': '',
                'å·¦è£¸çœ¼': '',
                'å·¦çŸ¯æ­£': '',
                'å³TOL': '',  # TOLæƒ…å ±è¿½åŠ 
                'å·¦TOL': '',  # TOLæƒ…å ±è¿½åŠ 
                'NCTå³': '',
                'NCTå·¦': '',
                'æ‰‹æ›¸ãå³': '',
                'æ‰‹æ›¸ãå·¦': '',
                'æœ€çµ‚çœ¼åœ§å³': '',
                'æœ€çµ‚çœ¼åœ§å·¦': '',
                'çœ¼åœ§å‚™è€ƒ': '',
                'ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿': '',
                'S': '',
                'C': '',
                'Ax': '',
                'æ‰‹è¡“æ—¥': '',
                'æ‚£è€…å': '',
                'è¡“å‰è¨ºæ–­': '',
                'è¡“å¼': '',
                'å¯¾è±¡çœ¼': '',
                'IOLåº¦æ•°_S': '',  # IOLã‚·ãƒ¼ãƒ«æƒ…å ±è¿½åŠ 
                'IOLåº¦æ•°_C': '',  # IOLã‚·ãƒ¼ãƒ«æƒ…å ±è¿½åŠ 
                'IOLåº¦æ•°_Ax': '',  # IOLã‚·ãƒ¼ãƒ«æƒ…å ±è¿½åŠ 
                'IOLè£½å“å': '',  # IOLã‚·ãƒ¼ãƒ«æƒ…å ±è¿½åŠ 
                'IOLãƒ¡ãƒ¼ã‚«ãƒ¼': '',  # IOLã‚·ãƒ¼ãƒ«æƒ…å ±è¿½åŠ 
                'IOLå‚™è€ƒ': '',  # IOLã‚·ãƒ¼ãƒ«æƒ…å ±è¿½åŠ 
                'æ¤œæŸ»ç¨®é¡': '',  # æ¤œæŸ»ç”»åƒè­˜åˆ¥è¿½åŠ 
                'æ¤œæŸ»è©³ç´°': '',  # æ¤œæŸ»ç”»åƒè­˜åˆ¥è¿½åŠ 
                'æ¤œæŸ»æ—¥': '',  # æ¤œæŸ»ç”»åƒè­˜åˆ¥è¿½åŠ 
                'æ¤œæŸ»å¯¾è±¡çœ¼': '',  # æ¤œæŸ»å¯¾è±¡çœ¼è¿½åŠ 
                'æ¤œæŸ»å‚™è€ƒ': '',  # æ¤œæŸ»ç”»åƒè­˜åˆ¥è¿½åŠ 
                'ocr_text': ''
            })
            continue
        
        print(f"  âœ… OCRæˆåŠŸ ({len(text)}æ–‡å­—)")
        
        # æœ€çµ‚åŒ…æ‹¬çš„ãªå‡¦ç†
        data = process_image_final_comprehensive(text, filename)
        
        # æŠ½å‡ºçµæœã®è©³ç´°è¡¨ç¤º
        print(f"  ğŸ“Š æŠ½å‡ºçµæœ:")
        if data['å³è£¸çœ¼'] or data['å·¦è£¸çœ¼']:
            print(f"    è¦–åŠ›: å³è£¸çœ¼={data['å³è£¸çœ¼'] or 'æœªæ¤œå‡º'}, å·¦è£¸çœ¼={data['å·¦è£¸çœ¼'] or 'æœªæ¤œå‡º'}")
        if data['å³TOL'] or data['å·¦TOL']:
            print(f"    TOL: å³={data['å³TOL'] or 'æœªæ¤œå‡º'}, å·¦={data['å·¦TOL'] or 'æœªæ¤œå‡º'}")
        if data['å³çŸ¯æ­£'] or data['å·¦çŸ¯æ­£']:
            print(f"    çŸ¯æ­£: å³={data['å³çŸ¯æ­£'] or 'æœªæ¤œå‡º'}, å·¦={data['å·¦çŸ¯æ­£'] or 'æœªæ¤œå‡º'}")
        if data['æœ€çµ‚çœ¼åœ§å³'] or data['æœ€çµ‚çœ¼åœ§å·¦']:
            print(f"    çœ¼åœ§: å³={data['æœ€çµ‚çœ¼åœ§å³'] or 'æœªæ¤œå‡º'}, å·¦={data['æœ€çµ‚çœ¼åœ§å·¦'] or 'æœªæ¤œå‡º'} ({data['ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿']})")
        else:
            print(f"    çœ¼åœ§: æœªæ¤œå‡º")
        if data['S'] or data['C'] or data['Ax']:
            print(f"    ãƒ¬ãƒ•å€¤: S={data['S'] or 'æœªæ¤œå‡º'}, C={data['C'] or 'æœªæ¤œå‡º'}, Ax={data['Ax'] or 'æœªæ¤œå‡º'}")
        else:
            print(f"    ãƒ¬ãƒ•å€¤: æœªæ¤œå‡º")
        if data['æ‰‹è¡“æ—¥'] or data['æ‚£è€…å'] or data['è¡“å‰è¨ºæ–­'] or data['è¡“å¼']:
            print(f"    æ‰‹è¡“æƒ…å ±: æ—¥={data['æ‰‹è¡“æ—¥'] or 'æœªæ¤œå‡º'}, æ‚£è€…={data['æ‚£è€…å'] or 'æœªæ¤œå‡º'}, è¨ºæ–­={data['è¡“å‰è¨ºæ–­'] or 'æœªæ¤œå‡º'}, å¯¾è±¡çœ¼={data['å¯¾è±¡çœ¼'] or 'æœªæ¤œå‡º'}, è¡“å¼={data['è¡“å¼'] or 'æœªæ¤œå‡º'}")
        else:
            print(f"    æ‰‹è¡“æƒ…å ±: æœªæ¤œå‡º")
        if data['IOLåº¦æ•°_S'] or data['IOLåº¦æ•°_C'] or data['IOLåº¦æ•°_Ax'] or data['IOLè£½å“å'] or data['IOLãƒ¡ãƒ¼ã‚«ãƒ¼']:
            print(f"    IOLã‚·ãƒ¼ãƒ«: S={data['IOLåº¦æ•°_S'] or 'æœªæ¤œå‡º'}, C={data['IOLåº¦æ•°_C'] or 'æœªæ¤œå‡º'}, Ax={data['IOLåº¦æ•°_Ax'] or 'æœªæ¤œå‡º'}, è£½å“={data['IOLè£½å“å'] or 'æœªæ¤œå‡º'}, ãƒ¡ãƒ¼ã‚«ãƒ¼={data['IOLãƒ¡ãƒ¼ã‚«ãƒ¼'] or 'æœªæ¤œå‡º'}")
        else:
            print(f"    IOLã‚·ãƒ¼ãƒ«: æœªæ¤œå‡º")
        if data['æ¤œæŸ»ç¨®é¡'] or data['æ¤œæŸ»è©³ç´°']:
            print(f"    æ¤œæŸ»ç”»åƒ: {data['æ¤œæŸ»ç¨®é¡'] or 'æœªæ¤œå‡º'} - {data['æ¤œæŸ»è©³ç´°'] or 'æœªæ¤œå‡º'} ({data['æ¤œæŸ»æ—¥'] or 'æœªæ¤œå‡º'}) - å¯¾è±¡çœ¼: {data['æ¤œæŸ»å¯¾è±¡çœ¼'] or 'æœªæ¤œå‡º'}")
            # æ¤œæŸ»ç¨®é¡åˆ¥ã®è©³ç´°ãƒ‡ãƒ¼ã‚¿è¡¨ç¤º
            if data['æ¤œæŸ»ç¨®é¡'] == 'OCT':
                if data.get('OCT_ç¶²è†œåš_å³') or data.get('OCT_ç¶²è†œåš_å·¦'):
                    print(f"    OCTæ•°å€¤: å³ç¶²è†œåš={data.get('OCT_ç¶²è†œåš_å³', 'æœªæ¤œå‡º')}, å·¦ç¶²è†œåš={data.get('OCT_ç¶²è†œåš_å·¦', 'æœªæ¤œå‡º')}")
            elif data['æ¤œæŸ»ç¨®é¡'] == 'OCTA':
                if data.get('OCTA_è¡€ç®¡å¯†åº¦_å³') or data.get('OCTA_è¡€ç®¡å¯†åº¦_å·¦'):
                    print(f"    OCTAæ•°å€¤: å³è¡€ç®¡å¯†åº¦={data.get('OCTA_è¡€ç®¡å¯†åº¦_å³', 'æœªæ¤œå‡º')}, å·¦è¡€ç®¡å¯†åº¦={data.get('OCTA_è¡€ç®¡å¯†åº¦_å·¦', 'æœªæ¤œå‡º')}")
            elif data['æ¤œæŸ»ç¨®é¡'] in ['ãƒãƒ³ãƒ•ãƒªãƒ¼è¦–é‡', 'AIMOè¦–é‡']:
                if data.get('è¦–é‡_MD_å³') or data.get('è¦–é‡_MD_å·¦'):
                    print(f"    è¦–é‡æ•°å€¤: å³MD={data.get('è¦–é‡_MD_å³', 'æœªæ¤œå‡º')}, å·¦MD={data.get('è¦–é‡_MD_å·¦', 'æœªæ¤œå‡º')}")
        else:
            print(f"    æ¤œæŸ»ç”»åƒ: æœªæ¤œå‡º")
        
        # çµæœã‚’è¨˜éŒ²
        result = {
            'filename': filename,
            'status': 'SUCCESS',
            'å³è£¸çœ¼': data['å³è£¸çœ¼'],
            'å³çŸ¯æ­£': data['å³çŸ¯æ­£'],
            'å·¦è£¸çœ¼': data['å·¦è£¸çœ¼'],
            'å·¦çŸ¯æ­£': data['å·¦çŸ¯æ­£'],
            'å³TOL': data['å³TOL'],  # TOLæƒ…å ±è¿½åŠ 
            'å·¦TOL': data['å·¦TOL'],  # TOLæƒ…å ±è¿½åŠ 
            'NCTå³': data['NCTå³'],
            'NCTå·¦': data['NCTå·¦'],
            'æ‰‹æ›¸ãå³': data['æ‰‹æ›¸ãå³'],
            'æ‰‹æ›¸ãå·¦': data['æ‰‹æ›¸ãå·¦'],
            'æœ€çµ‚çœ¼åœ§å³': data['æœ€çµ‚çœ¼åœ§å³'],
            'æœ€çµ‚çœ¼åœ§å·¦': data['æœ€çµ‚çœ¼åœ§å·¦'],
            'çœ¼åœ§å‚™è€ƒ': data['çœ¼åœ§å‚™è€ƒ'],
            'ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿': data['ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿'],
            'S': data['S'],
            'C': data['C'],
            'Ax': data['Ax'],
            'æ‰‹è¡“æ—¥': data['æ‰‹è¡“æ—¥'],
            'æ‚£è€…å': data['æ‚£è€…å'],
            'è¡“å‰è¨ºæ–­': data['è¡“å‰è¨ºæ–­'],
            'è¡“å¼': data['è¡“å¼'],
            'å¯¾è±¡çœ¼': data['å¯¾è±¡çœ¼'],
            'IOLåº¦æ•°_S': data['IOLåº¦æ•°_S'],  # IOLã‚·ãƒ¼ãƒ«æƒ…å ±è¿½åŠ 
            'IOLåº¦æ•°_C': data['IOLåº¦æ•°_C'],  # IOLã‚·ãƒ¼ãƒ«æƒ…å ±è¿½åŠ 
            'IOLåº¦æ•°_Ax': data['IOLåº¦æ•°_Ax'],  # IOLã‚·ãƒ¼ãƒ«æƒ…å ±è¿½åŠ 
            'IOLè£½å“å': data['IOLè£½å“å'],  # IOLã‚·ãƒ¼ãƒ«æƒ…å ±è¿½åŠ 
            'IOLãƒ¡ãƒ¼ã‚«ãƒ¼': data['IOLãƒ¡ãƒ¼ã‚«ãƒ¼'],  # IOLã‚·ãƒ¼ãƒ«æƒ…å ±è¿½åŠ 
            'IOLå‚™è€ƒ': data['IOLå‚™è€ƒ'],  # IOLã‚·ãƒ¼ãƒ«æƒ…å ±è¿½åŠ 
            'æ¤œæŸ»ç¨®é¡': data['æ¤œæŸ»ç¨®é¡'],  # æ¤œæŸ»ç”»åƒè­˜åˆ¥è¿½åŠ 
            'æ¤œæŸ»è©³ç´°': data['æ¤œæŸ»è©³ç´°'],  # æ¤œæŸ»ç”»åƒè­˜åˆ¥è¿½åŠ 
            'æ¤œæŸ»æ—¥': data['æ¤œæŸ»æ—¥'],  # æ¤œæŸ»ç”»åƒè­˜åˆ¥è¿½åŠ 
            'æ¤œæŸ»å¯¾è±¡çœ¼': data['æ¤œæŸ»å¯¾è±¡çœ¼'],  # æ¤œæŸ»å¯¾è±¡çœ¼è¿½åŠ 
            'æ¤œæŸ»å‚™è€ƒ': data['æ¤œæŸ»å‚™è€ƒ'],  # æ¤œæŸ»ç”»åƒè­˜åˆ¥è¿½åŠ 
            'ocr_text': text[:200] + "..." if len(text) > 200 else text
        }
        
        results.append(result)
    
    return results

def process_all_images_two_tier_comprehensive():
    """2æ®µæ§‹é€ å¯¾å¿œã®åŒ…æ‹¬çš„ã‚·ã‚¹ãƒ†ãƒ ã§å…¨ç”»åƒå‡¦ç†"""
    print("2æ®µæ§‹é€ å¯¾å¿œåŒ»ç™‚OCRã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 50)
    
    # Vision APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆ
    client = create_vision_client()
    if not client:
        print("âŒ Vision APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
        return []
    
    # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
    image_folder = r"C:\Projects\medical-ocr\inbox"
    image_files = glob.glob(os.path.join(image_folder, "*.JPG"))
    image_files.extend(glob.glob(os.path.join(image_folder, "*.jpg")))
    
    print(f"å‡¦ç†å¯¾è±¡ç”»åƒæ•°: {len(image_files)}")
    
    results = []
    
    for i, img_file in enumerate(image_files, 1):
        filename = os.path.basename(img_file)
        print(f"\n[{i}/{len(image_files)}] å‡¦ç†ä¸­: {filename}")
        
        # Google Vision APIå®Ÿè¡Œ
        text = google_vision_ocr(img_file, client)
        
        if not text:
            print(f"  âŒ OCRå¤±æ•—")
            results.append({
                'filename': filename,
                'status': 'OCR_FAILED',
                'å‰å›_å³è£¸çœ¼': '', 'å‰å›_å³çŸ¯æ­£': '', 'å‰å›_å·¦è£¸çœ¼': '', 'å‰å›_å·¦çŸ¯æ­£': '',
                'ä»Šå›_å³è£¸çœ¼': '', 'ä»Šå›_å³çŸ¯æ­£': '', 'ä»Šå›_å·¦è£¸çœ¼': '', 'ä»Šå›_å·¦çŸ¯æ­£': '',
                'ä»Šå›_S': '', 'ä»Šå›_C': '', 'ä»Šå›_A': '',
                'NCTå³': '', 'NCTå·¦': '', 'æ‰‹æ›¸ãå³': '', 'æ‰‹æ›¸ãå·¦': '',
                'æœ€çµ‚çœ¼åœ§å³': '', 'æœ€çµ‚çœ¼åœ§å·¦': '', 'çœ¼åœ§å‚™è€ƒ': '', 'ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿': '',
                'ocr_text': ''
            })
            continue
        
        print(f"  âœ… OCRæˆåŠŸ ({len(text)}æ–‡å­—)")
        
        # 2æ®µæ§‹é€ å¯¾å¿œã®åŒ…æ‹¬çš„å‡¦ç†
        data = process_image_two_tier_comprehensive(text)
        
        # æŠ½å‡ºçµæœã®è©³ç´°è¡¨ç¤º
        print(f"  ğŸ“Š æŠ½å‡ºçµæœ:")
        if data['ä»Šå›_å³è£¸çœ¼'] or data['ä»Šå›_å·¦è£¸çœ¼']:
            print(f"    ä»Šå›è¦–åŠ›: å³è£¸çœ¼={data['ä»Šå›_å³è£¸çœ¼'] or 'æœªæ¤œå‡º'}, å·¦è£¸çœ¼={data['ä»Šå›_å·¦è£¸çœ¼'] or 'æœªæ¤œå‡º'}")
        if data['ä»Šå›_å³çŸ¯æ­£'] or data['ä»Šå›_å·¦çŸ¯æ­£']:
            print(f"    ä»Šå›çŸ¯æ­£: å³={data['ä»Šå›_å³çŸ¯æ­£'] or 'æœªæ¤œå‡º'}, å·¦={data['ä»Šå›_å·¦çŸ¯æ­£'] or 'æœªæ¤œå‡º'}")
        if data['å‰å›_å³è£¸çœ¼'] or data['å‰å›_å·¦è£¸çœ¼']:
            print(f"    å‰å›è¦–åŠ›: å³è£¸çœ¼={data['å‰å›_å³è£¸çœ¼'] or 'æœªæ¤œå‡º'}, å·¦è£¸çœ¼={data['å‰å›_å·¦è£¸çœ¼'] or 'æœªæ¤œå‡º'}")
        if data['æœ€çµ‚çœ¼åœ§å³'] or data['æœ€çµ‚çœ¼åœ§å·¦']:
            print(f"    çœ¼åœ§: å³={data['æœ€çµ‚çœ¼åœ§å³'] or 'æœªæ¤œå‡º'}, å·¦={data['æœ€çµ‚çœ¼åœ§å·¦'] or 'æœªæ¤œå‡º'} ({data['ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿']})")
        else:
            print(f"    çœ¼åœ§: æœªæ¤œå‡º")
        
        # çµæœã‚’è¨˜éŒ²
        result = {
            'filename': filename,
            'status': 'SUCCESS',
            'å‰å›_å³è£¸çœ¼': data['å‰å›_å³è£¸çœ¼'],
            'å‰å›_å³çŸ¯æ­£': data['å‰å›_å³çŸ¯æ­£'],
            'å‰å›_å·¦è£¸çœ¼': data['å‰å›_å·¦è£¸çœ¼'],
            'å‰å›_å·¦çŸ¯æ­£': data['å‰å›_å·¦çŸ¯æ­£'],
            'ä»Šå›_å³è£¸çœ¼': data['ä»Šå›_å³è£¸çœ¼'],
            'ä»Šå›_å³çŸ¯æ­£': data['ä»Šå›_å³çŸ¯æ­£'],
            'ä»Šå›_å·¦è£¸çœ¼': data['ä»Šå›_å·¦è£¸çœ¼'],
            'ä»Šå›_å·¦çŸ¯æ­£': data['ä»Šå›_å·¦çŸ¯æ­£'],
            'ä»Šå›_S': data['ä»Šå›_S'],
            'ä»Šå›_C': data['ä»Šå›_C'],
            'ä»Šå›_A': data['ä»Šå›_A'],
            'NCTå³': data['NCTå³'],
            'NCTå·¦': data['NCTå·¦'],
            'æ‰‹æ›¸ãå³': data['æ‰‹æ›¸ãå³'],
            'æ‰‹æ›¸ãå·¦': data['æ‰‹æ›¸ãå·¦'],
            'æœ€çµ‚çœ¼åœ§å³': data['æœ€çµ‚çœ¼åœ§å³'],
            'æœ€çµ‚çœ¼åœ§å·¦': data['æœ€çµ‚çœ¼åœ§å·¦'],
            'çœ¼åœ§å‚™è€ƒ': data['çœ¼åœ§å‚™è€ƒ'],
            'ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿': data['ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿'],
            'ocr_text': text[:200] + "..." if len(text) > 200 else text
        }
        
        results.append(result)
    
    return results

def save_results_to_csv(results):
    """çµæœã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    csv_filename = f"fixed_vision_extraction_{timestamp}.csv"
    
    with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:
        fieldnames = [
            'filename', 'status', 'å³è£¸çœ¼', 'å³çŸ¯æ­£', 'å·¦è£¸çœ¼', 'å·¦çŸ¯æ­£', 'å³TOL', 'å·¦TOL',
            'NCTå³', 'NCTå·¦', 'æ‰‹æ›¸ãå³', 'æ‰‹æ›¸ãå·¦', 'æœ€çµ‚çœ¼åœ§å³', 'æœ€çµ‚çœ¼åœ§å·¦', 
            'çœ¼åœ§å‚™è€ƒ', 'ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿', 'S', 'C', 'Ax', 'æ‰‹è¡“æ—¥', 'æ‚£è€…å', 'è¡“å‰è¨ºæ–­', 'å¯¾è±¡çœ¼', 'è¡“å¼',
            'IOLåº¦æ•°_S', 'IOLåº¦æ•°_C', 'IOLåº¦æ•°_Ax', 'IOLè£½å“å', 'IOLãƒ¡ãƒ¼ã‚«ãƒ¼', 'IOLå‚™è€ƒ',
            'æ¤œæŸ»ç¨®é¡', 'æ¤œæŸ»è©³ç´°', 'æ¤œæŸ»æ—¥', 'æ¤œæŸ»å‚™è€ƒ', 'ocr_text'
        ]
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(results)
    
    print(f"\nâœ… çµæœã‚’ {csv_filename} ã«ä¿å­˜ã—ã¾ã—ãŸ")
    return csv_filename

def print_statistics(results):
    """çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º"""
    total_images = len(results)
    success_count = sum(1 for r in results if r['status'] == 'SUCCESS')
    vision_detected_count = sum(1 for r in results if any([
        r['å³è£¸çœ¼'], r['å³çŸ¯æ­£'], r['å·¦è£¸çœ¼'], r['å·¦çŸ¯æ­£']
    ]))
    
    print(f"\n=== çµ±è¨ˆæƒ…å ± ===")
    print(f"ç·ç”»åƒæ•°: {total_images}")
    print(f"OCRæˆåŠŸ: {success_count}")
    print(f"è¦–åŠ›ãƒ‡ãƒ¼ã‚¿æ¤œå‡º: {vision_detected_count}")
    print(f"è¦–åŠ›æ¤œå‡ºç‡: {vision_detected_count/total_images*100:.1f}%")

# ãƒ†ã‚¹ãƒˆç”¨ã®é–¢æ•°
def test_extraction():
    """å®Ÿéš›ã®ãƒ†ã‚­ã‚¹ãƒˆã§ãƒ†ã‚¹ãƒˆ"""
    test_text = """
V.d.=
0.01
x IOL
(n.c)
V.s.=
0.05
x IOL
(0.06 x S-1.75)
"""
    
    result = extract_vision_data_fixed(test_text)
    print("=== ãƒ†ã‚¹ãƒˆçµæœ ===")
    print(result)

def test_reconstruction():
    """è¡Œå†æ§‹ç¯‰ã®ãƒ†ã‚¹ãƒˆ"""
    test_text = """
V.d.=
0.01
x IOL
(n.c)
V.s.=
0.05
x IOL
(0.06 x S-1.75)
"""
    
    lines = test_text.split('\n')
    print("=== è¡Œå†æ§‹ç¯‰ãƒ†ã‚¹ãƒˆ ===")
    
    for i, line in enumerate(lines):
        if 'V.d.' in line:
            full_line = reconstruct_vision_line(lines, i)
            print(f"å†æ§‹ç¯‰ã•ã‚ŒãŸV.d.è¡Œ: {full_line}")
        elif 'V.s.' in line:
            full_line = reconstruct_vision_line(lines, i)
            print(f"å†æ§‹ç¯‰ã•ã‚ŒãŸV.s.è¡Œ: {full_line}")
    
    # æ”¹è‰¯ã•ã‚ŒãŸæŠ½å‡ºãƒ†ã‚¹ãƒˆ
    result = extract_vision_data_fixed(test_text)
    print(f"æŠ½å‡ºçµæœ: {result}")

def test_iop_extraction():
    """çœ¼åœ§æŠ½å‡ºã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆ"""
    test_cases = [
        "AT: 15ã€€18",
        "IOP 15/18",
        "ï¼¡ï¼´ã€€ï¼‘ï¼•ï¼ï¼‘ï¼˜",
        "iop 15ã€€18",
        "AT:15 18",
        "IOP 15  18",
        "AT: å³15 å·¦18",
        "IOP R15 L18",
        "çœ¼åœ§ 15/18",
        "AT 15, 18",
        "IOP: 15-18"
    ]
    
    print("=== çœ¼åœ§æŠ½å‡ºã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ ===")
    
    for i, test_text in enumerate(test_cases, 1):
        print(f"\nãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ {i}: {test_text}")
        result = extract_handwritten_iop_patterns_improved(test_text)
        print(f"çµæœ: {result}")
        
        if result['å³çœ¼åœ§'] and result['å·¦çœ¼åœ§']:
            print(f"âœ… æˆåŠŸ: R={result['å³çœ¼åœ§']}, L={result['å·¦çœ¼åœ§']}")
        else:
            print("âŒ å¤±æ•—")

def extract_handwritten_iop_patterns_improved(text):
    """æ‰‹æ›¸ãçœ¼åœ§æŠ½å‡ºï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
    
    lines = text.split('\n')
    result = {'å³çœ¼åœ§': '', 'å·¦çœ¼åœ§': '', 'çœ¼åœ§ãƒ¡ãƒ¢': ''}
    
    # æ”¹è‰¯ç‰ˆ: ã‚ˆã‚Šå¤šãã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œç´¢
    for line in lines:
        # DATEã®è¡Œã¯ã‚¹ã‚­ãƒƒãƒ—
        if 'DATE' in line.upper() or '2025/' in line or '2024/' in line:
            continue
            
        # çœ¼åœ§é–¢é€£ãƒãƒ¼ã‚«ãƒ¼ã‚’æ‹¡å¼µ
        if any(marker in line.upper() for marker in ['AT', 'IOP', 'ï¼¡ï¼´', 'ï¼©ï¼¯ï¼°', 'çœ¼åœ§', 'EYE', 'PRESSURE']):
            # DATEè¡Œã§ãªã„ã“ã¨ã‚’å†ç¢ºèª
            if 'DATE' not in line.upper():
                print(f"çœ¼åœ§è¡Œå€™è£œï¼ˆæ”¹è‰¯ç‰ˆï¼‰: {line}")
                
                # ãƒ‘ã‚¿ãƒ¼ãƒ³1: æ•°å­—ã®ã¿ï¼ˆ15 18ï¼‰
                numbers = re.findall(r'\b(\d{1,2})\b', line)
                valid_iop = [n for n in numbers if 0 <= int(n) <= 80]
                
                if len(valid_iop) >= 2:
                    result['å³çœ¼åœ§'] = valid_iop[0]
                    result['å·¦çœ¼åœ§'] = valid_iop[1]
                    result['çœ¼åœ§ãƒ¡ãƒ¢'] = f'ãƒ‘ã‚¿ãƒ¼ãƒ³1ï¼ˆæ•°å­—ã®ã¿ï¼‰: {line.strip()}'
                    return result
                
                # ãƒ‘ã‚¿ãƒ¼ãƒ³2: ã‚¹ãƒ©ãƒƒã‚·ãƒ¥å½¢å¼ï¼ˆ15/18ï¼‰
                slash = re.search(r'(\d{1,2})\s*[/ï¼]\s*(\d{1,2})', line)
                if slash:
                    v1, v2 = int(slash.group(1)), int(slash.group(2))
                    if 0 <= v1 <= 80 and 0 <= v2 <= 80:
                        result['å³çœ¼åœ§'] = str(v1)
                        result['å·¦çœ¼åœ§'] = str(v2)
                        result['çœ¼åœ§ãƒ¡ãƒ¢'] = f'ãƒ‘ã‚¿ãƒ¼ãƒ³2ï¼ˆã‚¹ãƒ©ãƒƒã‚·ãƒ¥ï¼‰: {line.strip()}'
                        return result
                
                # ãƒ‘ã‚¿ãƒ¼ãƒ³3: R/Lå½¢å¼ï¼ˆR15 L18ï¼‰
                rl_pattern = re.search(r'[Rï¼²]\s*(\d{1,2})\s*[Lï¼¬]\s*(\d{1,2})', line)
                if rl_pattern:
                    v1, v2 = int(rl_pattern.group(1)), int(rl_pattern.group(2))
                    if 0 <= v1 <= 80 and 0 <= v2 <= 80:
                        result['å³çœ¼åœ§'] = str(v1)
                        result['å·¦çœ¼åœ§'] = str(v2)
                        result['çœ¼åœ§ãƒ¡ãƒ¢'] = f'ãƒ‘ã‚¿ãƒ¼ãƒ³3ï¼ˆR/Lå½¢å¼ï¼‰: {line.strip()}'
                        return result
                
                # ãƒ‘ã‚¿ãƒ¼ãƒ³4: å³å·¦å½¢å¼ï¼ˆå³15 å·¦18ï¼‰
                right_left_pattern = re.search(r'å³\s*(\d{1,2})\s*å·¦\s*(\d{1,2})', line)
                if right_left_pattern:
                    v1, v2 = int(right_left_pattern.group(1)), int(right_left_pattern.group(2))
                    if 0 <= v1 <= 80 and 0 <= v2 <= 80:
                        result['å³çœ¼åœ§'] = str(v1)
                        result['å·¦çœ¼åœ§'] = str(v2)
                        result['çœ¼åœ§ãƒ¡ãƒ¢'] = f'ãƒ‘ã‚¿ãƒ¼ãƒ³4ï¼ˆå³å·¦å½¢å¼ï¼‰: {line.strip()}'
                        return result
    
    # æ‹¡å¼µæ¤œç´¢: çœ¼åœ§ãƒãƒ¼ã‚«ãƒ¼ãŒãªã„è¡Œã§ã‚‚æ•°å€¤ãƒšã‚¢ã‚’æ¢ã™
    print(f"  âš ï¸ çœ¼åœ§ãƒãƒ¼ã‚«ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚æ‹¡å¼µæ¤œç´¢...")
    
    for line in lines:
        # æ•°å€¤ãƒšã‚¢ã‚’æ¢ã™ï¼ˆçœ¼åœ§ã®å¯èƒ½æ€§ï¼‰
        numbers = re.findall(r'\b(\d{1,2})\b', line)
        valid_numbers = [n for n in numbers if 10 <= int(n) <= 30]  # çœ¼åœ§ã‚‰ã—ã„ç¯„å›²
        
        if len(valid_numbers) >= 2:
            # è¡Œã«çœ¼åœ§é–¢é€£ã®å˜èªãŒãªã„ã‹ãƒã‚§ãƒƒã‚¯
            if not any(word in line.upper() for word in ['DATE', '2025', '2024', 'TIME', 'å¹´', 'æœˆ', 'æ—¥']):
                result['å³çœ¼åœ§'] = valid_numbers[0]
                result['å·¦çœ¼åœ§'] = valid_numbers[1]
                result['çœ¼åœ§ãƒ¡ãƒ¢'] = f'æ‹¡å¼µæ¤œç´¢: {line.strip()}'
                print(f"  âœ… æ‹¡å¼µæ¤œç´¢ã§ç™ºè¦‹: {line.strip()}")
                return result
    
    return result

def extract_refraction_data(text):
    """ãƒ¬ãƒ•å€¤ï¼ˆå±ˆæŠ˜å€¤ï¼‰ã‚’æŠ½å‡ºï¼ˆãƒ—ãƒªãƒ³ãƒˆå‡ºåŠ›å¯¾å¿œï¼‰"""
    
    import re
    
    result = {
        'S': '',      # çƒé¢åº¦æ•° (Â±0.00ã€œÂ±30.00ã€å°æ•°ç‚¹ä»¥ä¸‹2æ¡)
        'C': '',      # å††æŸ±åº¦æ•° (Â±0.00ã€œÂ±30.00ã€å°æ•°ç‚¹ä»¥ä¸‹2æ¡)
        'Ax': ''      # è»¸ (0-360Â°ã€æ•´æ•°)
    }
    
    lines = text.split('\n')
    
    print("ğŸ” ãƒ¬ãƒ•å€¤æŠ½å‡ºä¸­...")
    
    # ãƒ¬ãƒ•å€¤é–¢é€£ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€è¡Œã‚’æ¢ã™
    refraction_keywords = ['REFRACTION', 'REFR', 'ãƒ¬ãƒ•', 'å±ˆæŠ˜', 'SPH', 'CYL', 'AXIS', 'Ax', 'AX']
    
    for i, line in enumerate(lines):
        line_upper = line.upper()
        
        # ãƒ¬ãƒ•å€¤é–¢é€£ã®è¡Œã‹ãƒã‚§ãƒƒã‚¯
        if any(keyword in line_upper for keyword in refraction_keywords):
            print(f"  ğŸ“„ ãƒ¬ãƒ•å€¤è¡Œå€™è£œ {i}: {line}")
            
            # Sï¼ˆçƒé¢åº¦æ•°ï¼‰ã‚’æ¢ã™ (Â±0.00ã€œÂ±30.00ã€å°æ•°ç‚¹ä»¥ä¸‹2æ¡)
            s_patterns = [
                r'SPH[:\s]*([+-]?\d+\.\d{2})',         # SPH: +1.25
                r'([+-]?\d+\.\d{2})\s*Ã—\s*S',          # +1.25Ã—S
                r'çƒé¢[:\s]*([+-]?\d+\.\d{2})',        # çƒé¢: +1.25
                r'([+-]?\d+\.\d{2})\s*çƒé¢',           # +1.25çƒé¢
                r'([+-]?\d+\.\d{2})\s+[+-]?\d+\.\d{2}\s+\d+',  # -0.75 -0.25 79 (æœ€åˆã®å€¤ãŒS)
                r'([+-]?\d+\.\d{2})\s+[+-]?\d+\.\d{2}\s+\d+\s*[0*]',  # -0.75 -0.25 79 0
            ]
            
            for pattern in s_patterns:
                match = re.search(pattern, line)
                if match:
                    s_value = match.group(1)
                    try:
                        s_float = float(s_value)
                        if -30.00 <= s_float <= 30.00:
                            result['S'] = s_value
                            print(f"    âœ… Så€¤ç™ºè¦‹: {s_value}")
                            break
                        else:
                            print(f"    âš ï¸ Så€¤ç¯„å›²å¤–: {s_value} (-30.00ã€œ+30.00)")
                    except:
                        continue
            
            # Cï¼ˆå††æŸ±åº¦æ•°ï¼‰ã‚’æ¢ã™ (Â±0.00ã€œÂ±30.00ã€å°æ•°ç‚¹ä»¥ä¸‹2æ¡)
            c_patterns = [
                r'CYL[:\s]*([+-]?\d+\.\d{2})',         # CYL: -0.50
                r'([+-]?\d+\.\d{2})\s*Ã—\s*C',          # -0.50Ã—C
                r'å††æŸ±[:\s]*([+-]?\d+\.\d{2})',        # å††æŸ±: -0.50
                r'([+-]?\d+\.\d{2})\s*å††æŸ±',           # -0.50å††æŸ±
                r'[+-]?\d+\.\d{2}\s+([+-]?\d+\.\d{2})\s+\d+',  # -0.75 -0.25 79 (2ç•ªç›®ã®å€¤ãŒC)
                r'[+-]?\d+\.\d{2}\s+([+-]?\d+\.\d{2})\s+\d+\s*[0*]',  # -0.75 -0.25 79 0
            ]
            
            for pattern in c_patterns:
                match = re.search(pattern, line)
                if match:
                    c_value = match.group(1)
                    try:
                        c_float = float(c_value)
                        if -30.00 <= c_float <= 30.00:
                            result['C'] = c_value
                            print(f"    âœ… Cå€¤ç™ºè¦‹: {c_value}")
                            break
                        else:
                            print(f"    âš ï¸ Cå€¤ç¯„å›²å¤–: {c_value} (-30.00ã€œ+30.00)")
                    except:
                        continue
            
            # Axï¼ˆè»¸ï¼‰ã‚’æ¢ã™ (0-360Â°ã€æ•´æ•°)
            ax_patterns = [
                r'AXIS[:\s]*(\d{1,3})',                # AXIS: 90
                r'Ax[:\s]*(\d{1,3})',                  # Ax: 90
                r'AX[:\s]*(\d{1,3})',                  # AX: 90
                r'è»¸[:\s]*(\d{1,3})',                  # è»¸: 90
                r'(\d{1,3})\s*åº¦',                     # 90åº¦
                r'(\d{1,3})\s*Â°',                      # 90Â°
                r'[+-]?\d+\.\d{2}\s+[+-]?\d+\.\d{2}\s+(\d{1,3})',  # -0.75 -0.25 79 (3ç•ªç›®ã®å€¤ãŒAx)
            ]
            
            for pattern in ax_patterns:
                match = re.search(pattern, line)
                if match:
                    ax_value = match.group(1)
                    try:
                        ax_int = int(ax_value)
                        if 0 <= ax_int <= 360:
                            result['Ax'] = ax_value
                            print(f"    âœ… Axå€¤ç™ºè¦‹: {ax_value}Â°")
                            break
                        else:
                            print(f"    âš ï¸ Axå€¤ç¯„å›²å¤–: {ax_value} (0-360Â°)")
                    except:
                        continue
    
    # SPH/CYL AXISè¡Œã®å¾Œã®æ•°å€¤è¡Œã‚’æ¢ã™ï¼ˆã‚ˆã‚Šæ­£ç¢ºãªæŠ½å‡ºï¼‰
    for i, line in enumerate(lines):
        line_upper = line.upper()
        
        # SPHè¡Œã‚’è¦‹ã¤ã‘ãŸã‚‰ã€æ¬¡ã®æ•°è¡Œã‚’ãƒã‚§ãƒƒã‚¯
        if 'SPH' in line_upper and i + 1 < len(lines):
            print(f"  ğŸ“„ SPHè¡Œç™ºè¦‹ {i}: {line}")
            
            # æ¬¡ã®æ•°è¡Œã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦æ•°å€¤ãƒšã‚¢ã‚’æ¢ã™
            for j in range(i + 1, min(i + 5, len(lines))):
                next_line = lines[j]
                print(f"    ğŸ“Š +{j-i}è¡Œç›®: {next_line}")
                
                # æ•°å€¤ãƒšã‚¢ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯ (-0.75 -0.25 79)
                pair_match = re.search(r'([+-]?\d+\.\d{2})\s+([+-]?\d+\.\d{2})\s+(\d{1,3})', next_line)
                if pair_match:
                    s_val, c_val, ax_val = pair_match.groups()
                    
                    # å€¤ã®ç¯„å›²ãƒã‚§ãƒƒã‚¯
                    try:
                        s_float = float(s_val)
                        c_float = float(c_val)
                        ax_int = int(ax_val)
                        
                        if -30.00 <= s_float <= 30.00 and -30.00 <= c_float <= 30.00 and 0 <= ax_int <= 360:
                            if not result['S']:
                                result['S'] = s_val
                                print(f"      âœ… Så€¤è¨­å®š: {s_val}")
                            if not result['C']:
                                result['C'] = c_val
                                print(f"      âœ… Cå€¤è¨­å®š: {c_val}")
                            if not result['Ax']:
                                result['Ax'] = ax_val
                                print(f"      âœ… Axå€¤è¨­å®š: {ax_val}Â°")
                            break
                    except:
                        continue
    
    # çµæœã‚’è¡¨ç¤º
    print(f"  ğŸ“Š ãƒ¬ãƒ•å€¤æŠ½å‡ºçµæœ:")
    print(f"    S: {result['S'] or 'æœªæ¤œå‡º'}")
    print(f"    C: {result['C'] or 'æœªæ¤œå‡º'}")
    print(f"    Ax: {result['Ax'] or 'æœªæ¤œå‡º'}")
    
    return result

def extract_degree_data(text):
    """åº¦æ•°æƒ…å ±ï¼ˆSã€Cã€Aï¼‰ã‚’æŠ½å‡º"""
    
    import re
    
    result = {'S': '', 'C': '', 'A': ''}
    
    # Sï¼ˆçƒé¢åº¦æ•°ï¼‰ã‚’æ¢ã™
    s_patterns = [
        r'S[:\s]*([+-]?\d+\.?\d*)',
        r'çƒé¢[:\s]*([+-]?\d+\.?\d*)',
        r'([+-]?\d+\.?\d*)\s*Ã—\s*S'
    ]
    
    for pattern in s_patterns:
        match = re.search(pattern, text)
        if match:
            result['S'] = match.group(1)
            break
    
    # Cï¼ˆå††æŸ±åº¦æ•°ï¼‰ã‚’æ¢ã™
    c_patterns = [
        r'C[:\s]*([+-]?\d+\.?\d*)',
        r'å††æŸ±[:\s]*([+-]?\d+\.?\d*)',
        r'([+-]?\d+\.?\d*)\s*Ã—\s*C'
    ]
    
    for pattern in c_patterns:
        match = re.search(pattern, text)
        if match:
            result['C'] = match.group(1)
            break
    
    # Aï¼ˆè»¸ï¼‰ã‚’æ¢ã™
    a_patterns = [
        r'A[:\s]*(\d+)',
        r'è»¸[:\s]*(\d+)',
        r'(\d+)\s*åº¦'
    ]
    
    for pattern in a_patterns:
        match = re.search(pattern, text)
        if match:
            result['A'] = match.group(1)
            break
    
    return result

def extract_surgery_data(text):
    """æ‰‹è¡“è¨˜éŒ²ã‹ã‚‰æ‰‹è¡“æƒ…å ±ã‚’æŠ½å‡º"""
    
    import re
    
    result = {
        'æ‰‹è¡“æ—¥': '',
        'æ‚£è€…å': '',
        'è¡“å‰è¨ºæ–­': '',
        'è¡“å¼': '',
        'å¯¾è±¡çœ¼': ''  # å³çœ¼ã€å·¦çœ¼ã€ä¸¡çœ¼
    }
    
    lines = text.split('\n')
    
    print("ğŸ” æ‰‹è¡“æƒ…å ±æŠ½å‡ºä¸­...")
    
    # æ‰‹è¡“æ—¥ã‚’æ¢ã™
    date_patterns = [
        r'æ‰‹è¡“æ—¥[:\s]*(\d{4}[å¹´/]\d{1,2}[æœˆ/]\d{1,2}[æ—¥]?)',
        r'(\d{4}[å¹´/]\d{1,2}[æœˆ/]\d{1,2}[æ—¥]?)\s*æ‰‹è¡“',
        r'DATE[:\s]*(\d{4}[/]\d{1,2}[/]\d{1,2})',
        r'(\d{4}[/]\d{1,2}[/]\d{1,2})\s*æ‰‹è¡“',
    ]
    
    for line in lines:
        for pattern in date_patterns:
            match = re.search(pattern, line)
            if match:
                result['æ‰‹è¡“æ—¥'] = match.group(1)
                print(f"  âœ… æ‰‹è¡“æ—¥ç™ºè¦‹: {result['æ‰‹è¡“æ—¥']}")
                break
        if result['æ‰‹è¡“æ—¥']:
            break
    
    # æ‚£è€…åã‚’æ¢ã™
    name_patterns = [
        r'æ‚£è€…æ°å[:\s]*([^\n]+)',
        r'æ‚£è€…å[:\s]*([^\n]+)',
        r'æ°å[:\s]*([^\n]+)',
        r'åå‰[:\s]*([^\n]+)',
    ]
    
    for line in lines:
        for pattern in name_patterns:
            match = re.search(pattern, line)
            if match:
                result['æ‚£è€…å'] = match.group(1).strip()
                print(f"  âœ… æ‚£è€…åç™ºè¦‹: {result['æ‚£è€…å']}")
                break
        if result['æ‚£è€…å']:
            break
    
    # è¡“å‰è¨ºæ–­ã‚’æ¢ã™ï¼ˆäº‹å‰å®šç¾©ãƒªã‚¹ãƒˆä½¿ç”¨ï¼‰
    diagnosis_patterns = [
        r'è¡“å‰è¨ºæ–­[:\s]*([^\n]+)',
        r'è¨ºæ–­[:\s]*([^\n]+)',
        r'ç—…å[:\s]*([^\n]+)',
    ]
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ã§è¡“å‰è¨ºæ–­ã‚’æ¢ã™
    for line in lines:
        for pattern in diagnosis_patterns:
            match = re.search(pattern, line)
            if match:
                raw_diagnosis = match.group(1).strip()
                result['è¡“å‰è¨ºæ–­'] = raw_diagnosis
                print(f"  âœ… è¡“å‰è¨ºæ–­ç™ºè¦‹: {result['è¡“å‰è¨ºæ–­']}")
                break
        if result['è¡“å‰è¨ºæ–­']:
            break
    
    # äº‹å‰å®šç¾©ãƒªã‚¹ãƒˆã§è¡“å‰è¨ºæ–­ã‚’åˆ†é¡ãƒ»æ¨™æº–åŒ–
    if result['è¡“å‰è¨ºæ–­']:
        raw_diagnosis = result['è¡“å‰è¨ºæ–­']  # å…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¿æŒ
        raw_text = result['è¡“å‰è¨ºæ–­'].upper()
        for category, keywords in PREDEFINED_DIAGNOSES.items():
            for keyword in keywords:
                if keyword.upper() in raw_text:
                    result['è¡“å‰è¨ºæ–­'] = category
                    print(f"  âœ… è¡“å‰è¨ºæ–­åˆ†é¡: {category}")
                    break
            if result['è¡“å‰è¨ºæ–­'] == category:
                break
        
        # å¯¾è±¡çœ¼ã‚’å…ƒã®è¨ºæ–­ã‹ã‚‰æŠ½å‡º
        if 'å³çœ¼' in raw_diagnosis:
            result['å¯¾è±¡çœ¼'] = 'å³çœ¼'
            print(f"  âœ… å¯¾è±¡çœ¼æŠ½å‡ºï¼ˆè¡“å‰è¨ºæ–­ï¼‰: å³çœ¼")
        elif 'å·¦çœ¼' in raw_diagnosis:
            result['å¯¾è±¡çœ¼'] = 'å·¦çœ¼'
            print(f"  âœ… å¯¾è±¡çœ¼æŠ½å‡ºï¼ˆè¡“å‰è¨ºæ–­ï¼‰: å·¦çœ¼")
        elif 'ä¸¡çœ¼' in raw_diagnosis:
            result['å¯¾è±¡çœ¼'] = 'ä¸¡çœ¼'
            print(f"  âœ… å¯¾è±¡çœ¼æŠ½å‡ºï¼ˆè¡“å‰è¨ºæ–­ï¼‰: ä¸¡çœ¼")
    
    # å¯¾è±¡çœ¼ã‚’æŠ½å‡ºï¼ˆå³çœ¼ã€å·¦çœ¼ã€ä¸¡çœ¼ï¼‰- è¡“å‰è¨ºæ–­ã‹ã‚‰å„ªå…ˆçš„ã«æŠ½å‡º
    if not result['å¯¾è±¡çœ¼']:
        eye_patterns = [
            r'å³çœ¼[:\s]*([^\n]*)',
            r'å·¦çœ¼[:\s]*([^\n]*)',
            r'ä¸¡çœ¼[:\s]*([^\n]*)',
            r'([å³å·¦ä¸¡]çœ¼)',
        ]
        
        for line in lines:
            for pattern in eye_patterns:
                match = re.search(pattern, line)
                if match:
                    eye_info = match.group(1).strip()
                    if 'å³çœ¼' in eye_info:
                        result['å¯¾è±¡çœ¼'] = 'å³çœ¼'
                    elif 'å·¦çœ¼' in eye_info:
                        result['å¯¾è±¡çœ¼'] = 'å·¦çœ¼'
                    elif 'ä¸¡çœ¼' in eye_info:
                        result['å¯¾è±¡çœ¼'] = 'ä¸¡çœ¼'
                    print(f"  âœ… å¯¾è±¡çœ¼ç™ºè¦‹: {result['å¯¾è±¡çœ¼']}")
                    break
            if result['å¯¾è±¡çœ¼']:
                break
    
    # è¡“å¼ã‚’æ¢ã™ï¼ˆäº‹å‰å®šç¾©ãƒªã‚¹ãƒˆä½¿ç”¨ï¼‰
    surgery_patterns = [
        r'äºˆå®šè¡“å¼[:\s]*([^\n]+)',
        r'å®Ÿæ–½æ‰‹è¡“[:\s]*([^\n]+)',
        r'è¡“å¼[:\s]*([^\n]+)',
        r'æ‰‹è¡“[:\s]*([^\n]+)',
        r'æ‰‹è¡“å[:\s]*([^\n]+)',
    ]
    
    # è¡“å¼ã¯è¤‡æ•°è¡Œã«ã‚ãŸã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã®ã§ã€è¡Œã‚’çµåˆã—ã¦æ¤œç´¢
    combined_text = ' '.join(lines)
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ã§è¡“å¼ã‚’æ¢ã™
    for pattern in surgery_patterns:
        match = re.search(pattern, combined_text)
        if match:
            raw_surgery = match.group(1).strip()
            result['è¡“å¼'] = raw_surgery
            print(f"  âœ… è¡“å¼ç™ºè¦‹: {result['è¡“å¼']}")
            break
    
    # äº‹å‰å®šç¾©ãƒªã‚¹ãƒˆã§è¡“å¼ã‚’åˆ†é¡ãƒ»æ¨™æº–åŒ–
    if result['è¡“å¼']:
        raw_text = result['è¡“å¼'].upper()
        for category, keywords in PREDEFINED_SURGERIES.items():
            for keyword in keywords:
                if keyword.upper() in raw_text:
                    result['è¡“å¼'] = category
                    print(f"  âœ… è¡“å¼åˆ†é¡: {category}")
                    break
            if result['è¡“å¼'] == category:
                break
    
    # è¡“å¼ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€äº‹å‰å®šç¾©ãƒªã‚¹ãƒˆã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§æ¤œç´¢
    if not result['è¡“å¼']:
        for line in lines:
            line_upper = line.upper()
            for category, keywords in PREDEFINED_SURGERIES.items():
                for keyword in keywords:
                    if keyword.upper() in line_upper:
                        result['è¡“å¼'] = category
                        print(f"  âœ… è¡“å¼ï¼ˆäº‹å‰å®šç¾©ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼‰ç™ºè¦‹: {category}")
                        break
                if result['è¡“å¼']:
                    break
            if result['è¡“å¼']:
                break
    
    # è¡“å¼ã®è©³ç´°åŒ–ï¼ˆéƒ¨åˆ†çš„ãªæƒ…å ±ã‹ã‚‰æ¨æ¸¬ï¼‰
    if result['è¡“å¼'] and len(result['è¡“å¼']) < 10:
        # çŸ­ã„è¡“å¼ã®å ´åˆã€å‘¨è¾ºã®è¡Œã‚‚ç¢ºèª
        for i, line in enumerate(lines):
            if result['è¡“å¼'] in line:
                # å‰å¾Œã®è¡Œã‚‚å«ã‚ã¦è¡“å¼ã‚’æ§‹ç¯‰
                context_lines = []
                for j in range(max(0, i-2), min(len(lines), i+3)):
                    if lines[j].strip() and not lines[j].strip().startswith('æ‰‹è¡“'):
                        context_lines.append(lines[j].strip())
                
                if len(context_lines) > 1:
                    result['è¡“å¼'] = ' '.join(context_lines[:3])  # æœ€å¤§3è¡Œã¾ã§
                    print(f"  âœ… è¡“å¼è©³ç´°åŒ–: {result['è¡“å¼']}")
                break
    
    # çµæœã‚’è¡¨ç¤º
    print(f"  ğŸ“Š æ‰‹è¡“æƒ…å ±æŠ½å‡ºçµæœ:")
    print(f"    æ‰‹è¡“æ—¥: {result['æ‰‹è¡“æ—¥'] or 'æœªæ¤œå‡º'}")
    print(f"    æ‚£è€…å: {result['æ‚£è€…å'] or 'æœªæ¤œå‡º'}")
    print(f"    è¡“å‰è¨ºæ–­: {result['è¡“å‰è¨ºæ–­'] or 'æœªæ¤œå‡º'}")
    print(f"    å¯¾è±¡çœ¼: {result['å¯¾è±¡çœ¼'] or 'æœªæ¤œå‡º'}")
    print(f"    è¡“å¼: {result['è¡“å¼'] or 'æœªæ¤œå‡º'}")
    
    return result

def extract_iol_seal_data(text):
    """IOLã‚·ãƒ¼ãƒ«ã‹ã‚‰åº¦æ•°ã¨è£½å“åã‚’æŠ½å‡º"""
    
    import re
    
    result = {
        'IOLåº¦æ•°_S': '',
        'IOLåº¦æ•°_C': '',
        'IOLåº¦æ•°_Ax': '',
        'IOLè£½å“å': '',
        'IOLãƒ¡ãƒ¼ã‚«ãƒ¼': '',
        'IOLå‚™è€ƒ': ''
    }
    
    lines = text.split('\n')
    
    # IOLã‚·ãƒ¼ãƒ«ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æ¢ã™
    iol_keywords = ['IOL', 'ã‚·ãƒ¼ãƒ«', 'ãƒ¬ãƒ³ã‚º', 'åº¦æ•°', 'è£½å“', 'ãƒ¡ãƒ¼ã‚«ãƒ¼', 'LENS', 'POWER', 'DIOPTER']
    
    iol_section = []
    for line in lines:
        if any(keyword in line.upper() for keyword in iol_keywords):
            iol_section.append(line)
    
    if not iol_section:
        return result
    
    # åº¦æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¢ã™ï¼ˆS, C, Axï¼‰
    for line in iol_section:
        # çƒé¢åº¦æ•°ï¼ˆSï¼‰
        s_pattern = re.search(r'S[:\s]*([+-]?\d+\.?\d*)', line)
        if s_pattern:
            result['IOLåº¦æ•°_S'] = s_pattern.group(1)
        
        # å††æŸ±åº¦æ•°ï¼ˆCï¼‰
        c_pattern = re.search(r'C[:\s]*([+-]?\d+\.?\d*)', line)
        if c_pattern:
            result['IOLåº¦æ•°_C'] = c_pattern.group(1)
        
        # è»¸ï¼ˆAxï¼‰
        ax_pattern = re.search(r'Ax[:\s]*(\d+)', line)
        if ax_pattern:
            result['IOLåº¦æ•°_Ax'] = ax_pattern.group(1)
    
    # è£½å“åãƒ»ãƒ¡ãƒ¼ã‚«ãƒ¼åã‚’æ¢ã™
    manufacturer_keywords = ['ALCON', 'AMO', 'JOHNSON', 'J&J', 'ZEISS', 'HOYA', 'CANON', 'NIDEK', 'TOPCON']
    
    for line in iol_section:
        # ãƒ¡ãƒ¼ã‚«ãƒ¼å
        for manufacturer in manufacturer_keywords:
            if manufacturer in line.upper():
                result['IOLãƒ¡ãƒ¼ã‚«ãƒ¼'] = manufacturer
                break
        
        # è£½å“åï¼ˆä¸€èˆ¬çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
        product_pattern = re.search(r'([A-Z]{2,}[A-Z0-9\s\-]+)', line)
        if product_pattern and len(product_pattern.group(1).strip()) > 2:
            potential_product = product_pattern.group(1).strip()
            if potential_product not in ['IOL', 'LENS', 'POWER', 'DIOPTER']:
                result['IOLè£½å“å'] = potential_product
    
    # å‚™è€ƒæ¬„ã«IOLã‚·ãƒ¼ãƒ«ã®æƒ…å ±ãŒã‚ã‚‹ã“ã¨ã‚’è¨˜éŒ²
    if any([result['IOLåº¦æ•°_S'], result['IOLåº¦æ•°_C'], result['IOLåº¦æ•°_Ax'], result['IOLè£½å“å'], result['IOLãƒ¡ãƒ¼ã‚«ãƒ¼']]):
        result['IOLå‚™è€ƒ'] = 'IOLã‚·ãƒ¼ãƒ«æƒ…å ±æ¤œå‡º'
    
    return result

def identify_examination_type(text):
    """æ¤œæŸ»ç”»åƒã®ç¨®é¡ã‚’è­˜åˆ¥ï¼ˆå¤ã„ç”»åƒå¯¾å¿œç‰ˆï¼‰"""
    
    import re
    
    result = {
        'æ¤œæŸ»ç¨®é¡': '',
        'æ¤œæŸ»è©³ç´°': '',
        'æ¤œæŸ»æ—¥': '',
        'æ¤œæŸ»å‚™è€ƒ': '',
        'å¯¾è±¡çœ¼': ''  # å·¦å³åˆ¤å®šã‚’è¿½åŠ 
    }
    
    lines = text.split('\n')
    text_upper = text.upper()
    
    # çœ¼åº•ã‚«ãƒ¡ãƒ©
    if any(keyword in text_upper for keyword in ['çœ¼åº•', 'FUNDUS', 'RETINAL', 'ã‚«ãƒ¡ãƒ©', 'CAMERA', 'çœ¼åº•å†™çœŸ']):
        result['æ¤œæŸ»ç¨®é¡'] = 'çœ¼åº•ã‚«ãƒ¡ãƒ©'
        result['æ¤œæŸ»è©³ç´°'] = 'çœ¼åº•å†™çœŸæ’®å½±'
        
        # çœ¼åº•ã‚«ãƒ¡ãƒ©ã®å·¦å³åˆ¤å®šï¼ˆè¦–ç¥çµŒä¹³é ­ã®ä½ç½®ã‹ã‚‰ï¼‰
        result['å¯¾è±¡çœ¼'] = determine_fundus_eye_side(text, text_upper)
        
        # æ¤œæŸ»æ—¥ã‚’æ¢ã™
        date_patterns = [
            r'(\d{4})[å¹´\-\/](\d{1,2})[æœˆ\-\/](\d{1,2})[æ—¥]?',
            r'(\d{1,2})[æœˆ\-\/](\d{1,2})[æ—¥\-\/](\d{4})',
            r'(\d{8})',  # YYYYMMDD
        ]
        
        for line in lines:
            for pattern in date_patterns:
                match = re.search(pattern, line)
                if match:
                    if len(match.groups()) == 3:
                        if len(match.group(1)) == 4:  # YYYY/MM/DD
                            result['æ¤œæŸ»æ—¥'] = f"{match.group(1)}-{match.group(2)}-{match.group(3)}"
                        else:  # MM/DD/YYYY
                            result['æ¤œæŸ»æ—¥'] = f"{match.group(3)}-{match.group(1)}-{match.group(2)}"
                    elif len(match.groups()) == 1:  # YYYYMMDD
                        date_str = match.group(1)
                        if len(date_str) == 8:
                            result['æ¤œæŸ»æ—¥'] = f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:8]}"
                    break
            if result['æ¤œæŸ»æ—¥']:
                break
    
    # OCTï¼ˆå…‰å¹²æ¸‰æ–­å±¤è¨ˆï¼‰
    elif any(keyword in text_upper for keyword in ['OCT', 'å…‰å¹²æ¸‰', 'æ–­å±¤', 'TOMOGRAPHY', 'TRITON', 'å…‰å¹²æ¸‰æ–­å±¤']):
        result['æ¤œæŸ»ç¨®é¡'] = 'OCT'
        result['æ¤œæŸ»è©³ç´°'] = 'å…‰å¹²æ¸‰æ–­å±¤è¨ˆæ¤œæŸ»'
        
        # OCTã®å·¦å³åˆ¤å®š
        result['å¯¾è±¡çœ¼'] = determine_eye_side_from_text(text, text_upper)
        
        # OCTã®è©³ç´°æƒ…å ±
        if 'MACULA' in text_upper or 'é»„æ–‘' in text:
            result['æ¤œæŸ»è©³ç´°'] = 'OCTï¼ˆé»„æ–‘éƒ¨ï¼‰'
        elif 'OPTIC' in text_upper or 'è¦–ç¥çµŒ' in text:
            result['æ¤œæŸ»è©³ç´°'] = 'OCTï¼ˆè¦–ç¥çµŒï¼‰'
        elif 'CORNEA' in text_upper or 'è§’è†œ' in text:
            result['æ¤œæŸ»è©³ç´°'] = 'OCTï¼ˆè§’è†œï¼‰'
        else:
            result['æ¤œæŸ»è©³ç´°'] = 'OCTï¼ˆç¶²è†œæ–­å±¤ï¼‰'
    
    # OCTAï¼ˆå…‰å¹²æ¸‰æ–­å±¤è¡€ç®¡é€ å½±ï¼‰
    elif any(keyword in text_upper for keyword in ['OCTA', 'è¡€ç®¡é€ å½±', 'ANGIOGRAPHY', 'ANGIO', 'è¡€ç®¡']):
        result['æ¤œæŸ»ç¨®é¡'] = 'OCTA'
        result['æ¤œæŸ»è©³ç´°'] = 'å…‰å¹²æ¸‰æ–­å±¤è¡€ç®¡é€ å½±'
        
        # OCTAã®å·¦å³åˆ¤å®š
        result['å¯¾è±¡çœ¼'] = determine_eye_side_from_text(text, text_upper)
        
        # OCTAã®è©³ç´°æƒ…å ±
        if 'MACULA' in text_upper or 'é»„æ–‘' in text:
            result['æ¤œæŸ»è©³ç´°'] = 'OCTAï¼ˆé»„æ–‘éƒ¨è¡€ç®¡ï¼‰'
        elif 'OPTIC' in text_upper or 'è¦–ç¥çµŒ' in text:
            result['æ¤œæŸ»è©³ç´°'] = 'OCTAï¼ˆè¦–ç¥çµŒè¡€ç®¡ï¼‰'
        else:
            result['æ¤œæŸ»è©³ç´°'] = 'OCTAï¼ˆç¶²è†œè¡€ç®¡ï¼‰'
    
    # AIMOè¦–é‡ï¼ˆæ˜ç¢ºã«AIMOã¨è­˜åˆ¥ã§ãã‚‹å ´åˆã®ã¿ï¼‰
    elif any(keyword in text_upper for keyword in ['AIMO', 'IMO', 'IMOè¦–é‡']):
        result['æ¤œæŸ»ç¨®é¡'] = 'AIMOè¦–é‡'
        result['æ¤œæŸ»è©³ç´°'] = 'AIMOè¦–é‡è¨ˆæ¤œæŸ»'
        
        # AIMOè¦–é‡ã®å·¦å³åˆ¤å®š
        result['å¯¾è±¡çœ¼'] = determine_eye_side_from_text(text, text_upper)
        
        # AIMOè¦–é‡ã®è©³ç´°
        if '30-2' in text_upper:
            result['æ¤œæŸ»è©³ç´°'] = 'AIMOè¦–é‡ï¼ˆ30-2ï¼‰'
        elif '24-2' in text_upper:
            result['æ¤œæŸ»è©³ç´°'] = 'AIMOè¦–é‡ï¼ˆ24-2ï¼‰'
        elif '10-2' in text_upper:
            result['æ¤œæŸ»è©³ç´°'] = 'AIMOè¦–é‡ï¼ˆ10-2ï¼‰'
    
    # ãƒãƒ³ãƒ•ãƒªãƒ¼è¦–é‡ï¼ˆè¦–é‡æ¤œæŸ»ã®ç‰¹å¾´çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼‰
    elif any(keyword in text_upper for keyword in ['HUMPHREY', 'ãƒãƒ³ãƒ•ãƒªãƒ¼', 'HFA', 'è¦–é‡è¨ˆ', 'è¦–é‡æ¤œæŸ»', 'PERIMETRY', 'VF', 'è¦–é‡æ¸¬å®š']):
        result['æ¤œæŸ»ç¨®é¡'] = 'ãƒãƒ³ãƒ•ãƒªãƒ¼è¦–é‡'
        result['æ¤œæŸ»è©³ç´°'] = 'ãƒãƒ³ãƒ•ãƒªãƒ¼è¦–é‡è¨ˆæ¤œæŸ»'
        
        # ãƒãƒ³ãƒ•ãƒªãƒ¼è¦–é‡ã®å·¦å³åˆ¤å®š
        result['å¯¾è±¡çœ¼'] = determine_eye_side_from_text(text, text_upper)
        
        # ãƒãƒ³ãƒ•ãƒªãƒ¼è¦–é‡ã®è©³ç´°
        if '30-2' in text_upper:
            result['æ¤œæŸ»è©³ç´°'] = 'ãƒãƒ³ãƒ•ãƒªãƒ¼è¦–é‡ï¼ˆ30-2ï¼‰'
        elif '24-2' in text_upper:
            result['æ¤œæŸ»è©³ç´°'] = 'ãƒãƒ³ãƒ•ãƒªãƒ¼è¦–é‡ï¼ˆ24-2ï¼‰'
        elif '10-2' in text_upper:
            result['æ¤œæŸ»è©³ç´°'] = 'ãƒãƒ³ãƒ•ãƒªãƒ¼è¦–é‡ï¼ˆ10-2ï¼‰'
        elif 'MACULA' in text_upper:
            result['æ¤œæŸ»è©³ç´°'] = 'ãƒãƒ³ãƒ•ãƒªãƒ¼è¦–é‡ï¼ˆé»„æ–‘éƒ¨ï¼‰'
        elif 'GLAUCOMA' in text_upper or 'ç·‘å†…éšœ' in text:
            result['æ¤œæŸ»è©³ç´°'] = 'ãƒãƒ³ãƒ•ãƒªãƒ¼è¦–é‡ï¼ˆç·‘å†…éšœï¼‰'
    
    # ãã®ä»–ã®æ¤œæŸ»ï¼ˆå¤ã„ç”»åƒå¯¾å¿œï¼‰
    else:
        # ä¸€èˆ¬çš„ãªæ¤œæŸ»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        if any(keyword in text_upper for keyword in ['æ¤œæŸ»', 'EXAMINATION', 'TEST', 'MEASUREMENT']):
            # è¦–é‡æ¤œæŸ»ã®å¯èƒ½æ€§ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆAIMOã§ãªã„å ´åˆï¼‰
            if any(keyword in text_upper for keyword in ['è¦–é‡', 'PERIMETRY', 'VF', 'è¦–åŠ›', 'VISION', 'è¦–é‡è¨ˆ', 'è¦–é‡æ¸¬å®š']):
                result['æ¤œæŸ»ç¨®é¡'] = 'ãƒãƒ³ãƒ•ãƒªãƒ¼è¦–é‡'
                result['æ¤œæŸ»è©³ç´°'] = 'ãƒãƒ³ãƒ•ãƒªãƒ¼è¦–é‡è¨ˆæ¤œæŸ»'
                result['å¯¾è±¡çœ¼'] = determine_eye_side_from_text(text, text_upper)
                
                # ãƒãƒ³ãƒ•ãƒªãƒ¼è¦–é‡ã®è©³ç´°
                if '30-2' in text_upper:
                    result['æ¤œæŸ»è©³ç´°'] = 'ãƒãƒ³ãƒ•ãƒªãƒ¼è¦–é‡ï¼ˆ30-2ï¼‰'
                elif '24-2' in text_upper:
                    result['æ¤œæŸ»è©³ç´°'] = 'ãƒãƒ³ãƒ•ãƒªãƒ¼è¦–é‡ï¼ˆ24-2ï¼‰'
                elif '10-2' in text_upper:
                    result['æ¤œæŸ»è©³ç´°'] = 'ãƒãƒ³ãƒ•ãƒªãƒ¼è¦–é‡ï¼ˆ10-2ï¼‰'
                elif 'MACULA' in text_upper:
                    result['æ¤œæŸ»è©³ç´°'] = 'ãƒãƒ³ãƒ•ãƒªãƒ¼è¦–é‡ï¼ˆé»„æ–‘éƒ¨ï¼‰'
                elif 'GLAUCOMA' in text_upper or 'ç·‘å†…éšœ' in text:
                    result['æ¤œæŸ»è©³ç´°'] = 'ãƒãƒ³ãƒ•ãƒªãƒ¼è¦–é‡ï¼ˆç·‘å†…éšœï¼‰'
            # çœ¼åº•æ¤œæŸ»ã®å¯èƒ½æ€§
            elif any(keyword in text_upper for keyword in ['çœ¼åº•', 'FUNDUS', 'RETINAL', 'ç¶²è†œ', 'è¦–ç¥çµŒä¹³é ­']):
                result['æ¤œæŸ»ç¨®é¡'] = 'çœ¼åº•ã‚«ãƒ¡ãƒ©'
                result['æ¤œæŸ»è©³ç´°'] = 'çœ¼åº•å†™çœŸæ’®å½±'
                result['å¯¾è±¡çœ¼'] = determine_fundus_eye_side(text, text_upper)
            # OCTæ¤œæŸ»ã®å¯èƒ½æ€§
            elif any(keyword in text_upper for keyword in ['æ–­å±¤', 'TOMOGRAPHY', 'ç¶²è†œåš', 'é»„æ–‘åš']):
                result['æ¤œæŸ»ç¨®é¡'] = 'OCT'
                result['æ¤œæŸ»è©³ç´°'] = 'OCTï¼ˆç¶²è†œæ–­å±¤ï¼‰'
                result['å¯¾è±¡çœ¼'] = determine_eye_side_from_text(text, text_upper)
            else:
                result['æ¤œæŸ»ç¨®é¡'] = 'ãã®ä»–æ¤œæŸ»'
                result['æ¤œæŸ»è©³ç´°'] = 'ä¸€èˆ¬æ¤œæŸ»'
                result['å¯¾è±¡çœ¼'] = determine_eye_side_from_text(text, text_upper)
        else:
            result['æ¤œæŸ»ç¨®é¡'] = 'æœªåˆ†é¡'
            result['æ¤œæŸ»è©³ç´°'] = 'æ¤œæŸ»ç¨®é¡ä¸æ˜'
            result['å¯¾è±¡çœ¼'] = ''
    
    # æ¤œæŸ»å‚™è€ƒã«è¿½åŠ æƒ…å ±ã‚’è¨˜éŒ²
    additional_info = []
    
    # æ¤œæŸ»ã®è³ªã«é–¢ã™ã‚‹æƒ…å ±
    if any(keyword in text_upper for keyword in ['GOOD', 'è‰¯å¥½', 'OK']):
        additional_info.append('æ¤œæŸ»è³ªè‰¯å¥½')
    elif any(keyword in text_upper for keyword in ['POOR', 'ä¸è‰¯', 'NG', 'FAIL']):
        additional_info.append('æ¤œæŸ»è³ªä¸è‰¯')
    
    # æ¤œæŸ»ã®ä¿¡é ¼æ€§
    if any(keyword in text_upper for keyword in ['RELIABLE', 'ä¿¡é ¼', 'VALID']):
        additional_info.append('ä¿¡é ¼æ€§é«˜')
    elif any(keyword in text_upper for keyword in ['UNRELIABLE', 'ä¸ä¿¡é ¼', 'INVALID']):
        additional_info.append('ä¿¡é ¼æ€§ä½')
    
    # æ¤œæŸ»ã®å®Œäº†çŠ¶æ³
    if any(keyword in text_upper for keyword in ['COMPLETE', 'å®Œäº†', 'FINISH']):
        additional_info.append('æ¤œæŸ»å®Œäº†')
    elif any(keyword in text_upper for keyword in ['INCOMPLETE', 'æœªå®Œäº†', 'INCOMPLETE']):
        additional_info.append('æ¤œæŸ»æœªå®Œäº†')
    
    # å¤ã„ç”»åƒã®å¯èƒ½æ€§
    if any(keyword in text_upper for keyword in ['OLD', 'å¤ã„', '2011', '2012', '2013', '2014', '2015', '2016', '2017']):
        additional_info.append('å¤ã„ç”»åƒ')
    
    if additional_info:
        result['æ¤œæŸ»å‚™è€ƒ'] = ', '.join(additional_info)
    
    return result

def determine_fundus_eye_side(text, text_upper):
    """çœ¼åº•ã‚«ãƒ¡ãƒ©ã®å·¦å³åˆ¤å®šï¼ˆè¦–ç¥çµŒä¹³é ­ã®ä½ç½®ã‹ã‚‰ï¼‰"""
    
    # æ˜ç¤ºçš„ãªå·¦å³è¡¨è¨˜ã‚’ãƒã‚§ãƒƒã‚¯
    if any(keyword in text_upper for keyword in ['RIGHT', 'å³', 'R']):
        return 'Right'
    elif any(keyword in text_upper for keyword in ['LEFT', 'å·¦', 'L']):
        return 'Left'
    elif any(keyword in text_upper for keyword in ['BOTH', 'ä¸¡çœ¼', 'ä¸¡æ–¹']):
        return 'Both'
    
    # è¦–ç¥çµŒä¹³é ­ã®ä½ç½®ã‹ã‚‰åˆ¤å®š
    # å³çœ¼ã®çœ¼åº•å†™çœŸï¼šè¦–ç¥çµŒä¹³é ­ãŒå·¦å´ï¼ˆé¼»å´ï¼‰ã«ä½ç½®
    # å·¦çœ¼ã®çœ¼åº•å†™çœŸï¼šè¦–ç¥çµŒä¹³é ­ãŒå³å´ï¼ˆé¼»å´ï¼‰ã«ä½ç½®
    
    # è¦–ç¥çµŒä¹³é ­é–¢é€£ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    optic_keywords = ['è¦–ç¥çµŒä¹³é ­', 'OPTIC NERVE', 'OPTIC DISC', 'è¦–ç¥çµŒ', 'ä¹³é ­']
    
    # ä½ç½®ã‚’ç¤ºã™ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    left_position_keywords = ['å·¦å´', 'LEFT', 'L SIDE', 'é¼»å´', 'NASAL']
    right_position_keywords = ['å³å´', 'RIGHT', 'R SIDE', 'è€³å´', 'TEMPORAL']
    
    # è¦–ç¥çµŒä¹³é ­ãŒå·¦å´ã«ã‚ã‚‹å ´åˆï¼ˆå³çœ¼ã®çœ¼åº•å†™çœŸï¼‰
    if any(keyword in text for keyword in optic_keywords) and any(keyword in text for keyword in left_position_keywords):
        return 'Right'
    
    # è¦–ç¥çµŒä¹³é ­ãŒå³å´ã«ã‚ã‚‹å ´åˆï¼ˆå·¦çœ¼ã®çœ¼åº•å†™çœŸï¼‰
    if any(keyword in text for keyword in optic_keywords) and any(keyword in text for keyword in right_position_keywords):
        return 'Left'
    
    # é»„æ–‘ã®ä½ç½®ã‹ã‚‰åˆ¤å®š
    macula_keywords = ['é»„æ–‘', 'MACULA', 'ä¸­å¿ƒçª©']
    
    # é»„æ–‘ãŒå³å´ã«ã‚ã‚‹å ´åˆï¼ˆå³çœ¼ã®çœ¼åº•å†™çœŸï¼‰
    if any(keyword in text for keyword in macula_keywords) and any(keyword in text for keyword in right_position_keywords):
        return 'Right'
    
    # é»„æ–‘ãŒå·¦å´ã«ã‚ã‚‹å ´åˆï¼ˆå·¦çœ¼ã®çœ¼åº•å†™çœŸï¼‰
    if any(keyword in text for keyword in macula_keywords) and any(keyword in text for keyword in left_position_keywords):
        return 'Left'
    
    # è¡€ç®¡ã®èµ°è¡Œã‹ã‚‰åˆ¤å®šï¼ˆä¸Šãƒ»ä¸‹è¡€ç®¡å¼“ã®ä½ç½®ï¼‰
    vessel_keywords = ['è¡€ç®¡', 'VESSEL', 'å‹•è„ˆ', 'é™è„ˆ', 'ARTERY', 'VEIN']
    
    # ä¸Šè¡€ç®¡å¼“ãŒä¸Šå´ã«ã‚ã‚‹å ´åˆï¼ˆå³çœ¼ã®çœ¼åº•å†™çœŸï¼‰
    if any(keyword in text for keyword in vessel_keywords) and 'ä¸Š' in text and 'ä¸Šå´' in text:
        return 'Right'
    
    # ä¸Šè¡€ç®¡å¼“ãŒä¸‹å´ã«ã‚ã‚‹å ´åˆï¼ˆå·¦çœ¼ã®çœ¼åº•å†™çœŸï¼‰
    if any(keyword in text for keyword in vessel_keywords) and 'ä¸Š' in text and 'ä¸‹å´' in text:
        return 'Left'
    
    # åˆ¤å®šä¸èƒ½
    return 'Unknown'

def determine_eye_side_from_text(text, text_upper):
    """ä¸€èˆ¬çš„ãªæ¤œæŸ»ã®å·¦å³åˆ¤å®š"""
    
    # æ˜ç¤ºçš„ãªå·¦å³è¡¨è¨˜ã‚’ãƒã‚§ãƒƒã‚¯
    if any(keyword in text_upper for keyword in ['RIGHT', 'å³', 'R']):
        return 'Right'
    elif any(keyword in text_upper for keyword in ['LEFT', 'å·¦', 'L']):
        return 'Left'
    elif any(keyword in text_upper for keyword in ['BOTH', 'ä¸¡çœ¼', 'ä¸¡æ–¹', 'BILATERAL']):
        return 'Both'
    
    # åˆ¤å®šä¸èƒ½
    return 'Unknown'

def extract_oct_data(text, text_upper):
    """OCTã‹ã‚‰è©³ç´°æƒ…å ±ã‚’æŠ½å‡º"""
    
    result = {
        'OCT_ç¶²è†œåš_å³': '',
        'OCT_ç¶²è†œåš_å·¦': '',
        'OCT_é»„æ–‘åš_å³': '',
        'OCT_é»„æ–‘åš_å·¦': '',
        'OCT_è¦–ç¥çµŒåš_å³': '',
        'OCT_è¦–ç¥çµŒåš_å·¦': '',
        'OCT_ç•°å¸¸æ‰€è¦‹': '',
        'OCT_å‚™è€ƒ': ''
    }
    
    # ç¶²è†œåšã®æŠ½å‡º
    thickness_patterns = [
        r'(\d+\.?\d*)\s*(?:Î¼m|um|ãƒŸã‚¯ãƒ­ãƒ³|ãƒã‚¤ã‚¯ãƒ­ãƒ¡ãƒ¼ãƒˆãƒ«)',
        r'åš[åº¦ã¿]\s*[ï¼š:]\s*(\d+\.?\d*)',
        r'THICKNESS[:\s]*(\d+\.?\d*)',
        r'åšã¿\s*(\d+\.?\d*)'
    ]
    
    # å³çœ¼ã®ç¶²è†œåš
    for line in text.split('\n'):
        if any(keyword in line.upper() for keyword in ['RIGHT', 'å³', 'R']):
            for pattern in thickness_patterns:
                match = re.search(pattern, line, re.IGNORECASE)
                if match:
                    result['OCT_ç¶²è†œåš_å³'] = match.group(1)
                    break
    
    # å·¦çœ¼ã®ç¶²è†œåš
    for line in text.split('\n'):
        if any(keyword in line.upper() for keyword in ['LEFT', 'å·¦', 'L']):
            for pattern in thickness_patterns:
                match = re.search(pattern, line, re.IGNORECASE)
                if match:
                    result['OCT_ç¶²è†œåš_å·¦'] = match.group(1)
                    break
    
    # é»„æ–‘åšã®æŠ½å‡º
    macula_patterns = [
        r'é»„æ–‘[åšã¿]\s*[ï¼š:]\s*(\d+\.?\d*)',
        r'MACULA[:\s]*(\d+\.?\d*)',
        r'é»„æ–‘éƒ¨\s*(\d+\.?\d*)'
    ]
    
    for line in text.split('\n'):
        if 'é»„æ–‘' in line or 'MACULA' in line.upper():
            for pattern in macula_patterns:
                match = re.search(pattern, line, re.IGNORECASE)
                if match:
                    if any(keyword in line.upper() for keyword in ['RIGHT', 'å³', 'R']):
                        result['OCT_é»„æ–‘åš_å³'] = match.group(1)
                    elif any(keyword in line.upper() for keyword in ['LEFT', 'å·¦', 'L']):
                        result['OCT_é»„æ–‘åš_å·¦'] = match.group(1)
                    break
    
    # è¦–ç¥çµŒåšã®æŠ½å‡º
    optic_patterns = [
        r'è¦–ç¥çµŒ[åšã¿]\s*[ï¼š:]\s*(\d+\.?\d*)',
        r'OPTIC[:\s]*(\d+\.?\d*)',
        r'ä¹³é ­[åšã¿]\s*(\d+\.?\d*)'
    ]
    
    for line in text.split('\n'):
        if 'è¦–ç¥çµŒ' in line or 'OPTIC' in line.upper() or 'ä¹³é ­' in line:
            for pattern in optic_patterns:
                match = re.search(pattern, line, re.IGNORECASE)
                if match:
                    if any(keyword in line.upper() for keyword in ['RIGHT', 'å³', 'R']):
                        result['OCT_è¦–ç¥çµŒåš_å³'] = match.group(1)
                    elif any(keyword in line.upper() for keyword in ['LEFT', 'å·¦', 'L']):
                        result['OCT_è¦–ç¥çµŒåš_å·¦'] = match.group(1)
                    break
    
    # ç•°å¸¸æ‰€è¦‹ã®æŠ½å‡º
    abnormal_keywords = ['æµ®è…«', 'èç¸®', 'å‰¥é›¢', 'å‡ºè¡€', 'æ»²å‡º', 'EDEMA', 'ATROPHY', 'DETACHMENT', 'HEMORRHAGE', 'EXUDATE']
    abnormal_findings = []
    
    for line in text.split('\n'):
        for keyword in abnormal_keywords:
            if keyword in line:
                abnormal_findings.append(line.strip())
                break
    
    if abnormal_findings:
        result['OCT_ç•°å¸¸æ‰€è¦‹'] = '; '.join(abnormal_findings[:3])  # æœ€å¤§3ã¤ã¾ã§
    
    # å‚™è€ƒã®è¨­å®š
    if any([result['OCT_ç¶²è†œåš_å³'], result['OCT_ç¶²è†œåš_å·¦'], result['OCT_é»„æ–‘åš_å³'], result['OCT_é»„æ–‘åš_å·¦']]):
        result['OCT_å‚™è€ƒ'] = 'OCTæ•°å€¤ãƒ‡ãƒ¼ã‚¿æ¤œå‡º'
    
    return result

def extract_octa_data(text, text_upper):
    """OCTAã‹ã‚‰è©³ç´°æƒ…å ±ã‚’æŠ½å‡º"""
    
    result = {
        'OCTA_è¡€ç®¡å¯†åº¦_å³': '',
        'OCTA_è¡€ç®¡å¯†åº¦_å·¦': '',
        'OCTA_è¡€æµé€Ÿåº¦_å³': '',
        'OCTA_è¡€æµé€Ÿåº¦_å·¦': '',
        'OCTA_è¡€ç®¡ç•°å¸¸': '',
        'OCTA_å‚™è€ƒ': ''
    }
    
    # è¡€ç®¡å¯†åº¦ã®æŠ½å‡º
    density_patterns = [
        r'è¡€ç®¡å¯†åº¦[ï¼š:]\s*(\d+\.?\d*)',
        r'VESSEL\s*DENSITY[:\s]*(\d+\.?\d*)',
        r'å¯†åº¦[ï¼š:]\s*(\d+\.?\d*)'
    ]
    
    for line in text.split('\n'):
        if 'è¡€ç®¡å¯†åº¦' in line or 'VESSEL DENSITY' in line.upper() or 'å¯†åº¦' in line:
            for pattern in density_patterns:
                match = re.search(pattern, line, re.IGNORECASE)
                if match:
                    if any(keyword in line.upper() for keyword in ['RIGHT', 'å³', 'R']):
                        result['OCTA_è¡€ç®¡å¯†åº¦_å³'] = match.group(1)
                    elif any(keyword in line.upper() for keyword in ['LEFT', 'å·¦', 'L']):
                        result['OCTA_è¡€ç®¡å¯†åº¦_å·¦'] = match.group(1)
                    break
    
    # è¡€æµé€Ÿåº¦ã®æŠ½å‡º
    flow_patterns = [
        r'è¡€æµ[é€Ÿåº¦]\s*[ï¼š:]\s*(\d+\.?\d*)',
        r'FLOW[:\s]*(\d+\.?\d*)',
        r'é€Ÿåº¦[ï¼š:]\s*(\d+\.?\d*)'
    ]
    
    for line in text.split('\n'):
        if 'è¡€æµ' in line or 'FLOW' in line.upper() or 'é€Ÿåº¦' in line:
            for pattern in flow_patterns:
                match = re.search(pattern, line, re.IGNORECASE)
                if match:
                    if any(keyword in line.upper() for keyword in ['RIGHT', 'å³', 'R']):
                        result['OCTA_è¡€æµé€Ÿåº¦_å³'] = match.group(1)
                    elif any(keyword in line.upper() for keyword in ['LEFT', 'å·¦', 'L']):
                        result['OCTA_è¡€æµé€Ÿåº¦_å·¦'] = match.group(1)
                    break
    
    # è¡€ç®¡ç•°å¸¸ã®æŠ½å‡º
    abnormal_keywords = ['æ–°ç”Ÿè¡€ç®¡', 'è¡€ç®¡é–‰å¡', 'è¡€ç®¡æ‹¡å¼µ', 'NEOVASCULARIZATION', 'OCCLUSION', 'DILATION', 'è¡€ç®¡ç•°å¸¸']
    abnormal_findings = []
    
    for line in text.split('\n'):
        for keyword in abnormal_keywords:
            if keyword in line:
                abnormal_findings.append(line.strip())
                break
    
    if abnormal_findings:
        result['OCTA_è¡€ç®¡ç•°å¸¸'] = '; '.join(abnormal_findings[:3])  # æœ€å¤§3ã¤ã¾ã§
    
    # å‚™è€ƒã®è¨­å®š
    if any([result['OCTA_è¡€ç®¡å¯†åº¦_å³'], result['OCTA_è¡€ç®¡å¯†åº¦_å·¦'], result['OCTA_è¡€æµé€Ÿåº¦_å³'], result['OCTA_è¡€æµé€Ÿåº¦_å·¦']]):
        result['OCTA_å‚™è€ƒ'] = 'OCTAæ•°å€¤ãƒ‡ãƒ¼ã‚¿æ¤œå‡º'
    
    return result

def extract_visual_field_data(text, text_upper):
    """è¦–é‡æ¤œæŸ»ã‹ã‚‰è©³ç´°æƒ…å ±ã‚’æŠ½å‡º"""
    
    result = {
        'è¦–é‡_MD_å³': '',
        'è¦–é‡_MD_å·¦': '',
        'è¦–é‡_PSD_å³': '',
        'è¦–é‡_PSD_å·¦': '',
        'è¦–é‡_æ„Ÿåº¦_å³': '',
        'è¦–é‡_æ„Ÿåº¦_å·¦': '',
        'è¦–é‡_å›ºè¦–æå¤±': '',
        'è¦–é‡_å½é™½æ€§': '',
        'è¦–é‡_å½é™°æ€§': '',
        'è¦–é‡_ç•°å¸¸æ‰€è¦‹': '',
        'è¦–é‡_å‚™è€ƒ': ''
    }
    
    # MDï¼ˆå¹³å‡åå·®ï¼‰ã®æŠ½å‡º
    md_patterns = [
        r'MD[:\s]*([+-]?\d+\.?\d*)',
        r'å¹³å‡åå·®[ï¼š:]\s*([+-]?\d+\.?\d*)',
        r'MEAN\s*DEVIATION[:\s]*([+-]?\d+\.?\d*)'
    ]
    
    for line in text.split('\n'):
        if 'MD' in line.upper() or 'å¹³å‡åå·®' in line or 'MEAN DEVIATION' in line.upper():
            for pattern in md_patterns:
                match = re.search(pattern, line, re.IGNORECASE)
                if match:
                    if any(keyword in line.upper() for keyword in ['RIGHT', 'å³', 'R']):
                        result['è¦–é‡_MD_å³'] = match.group(1)
                    elif any(keyword in line.upper() for keyword in ['LEFT', 'å·¦', 'L']):
                        result['è¦–é‡_MD_å·¦'] = match.group(1)
                    break
    
    # PSDï¼ˆãƒ‘ã‚¿ãƒ¼ãƒ³æ¨™æº–åå·®ï¼‰ã®æŠ½å‡º
    psd_patterns = [
        r'PSD[:\s]*(\d+\.?\d*)',
        r'ãƒ‘ã‚¿ãƒ¼ãƒ³æ¨™æº–åå·®[ï¼š:]\s*(\d+\.?\d*)',
        r'PATTERN\s*STANDARD\s*DEVIATION[:\s]*(\d+\.?\d*)'
    ]
    
    for line in text.split('\n'):
        if 'PSD' in line.upper() or 'ãƒ‘ã‚¿ãƒ¼ãƒ³æ¨™æº–åå·®' in line or 'PATTERN STANDARD DEVIATION' in line.upper():
            for pattern in psd_patterns:
                match = re.search(pattern, line, re.IGNORECASE)
                if match:
                    if any(keyword in line.upper() for keyword in ['RIGHT', 'å³', 'R']):
                        result['è¦–é‡_PSD_å³'] = match.group(1)
                    elif any(keyword in line.upper() for keyword in ['LEFT', 'å·¦', 'L']):
                        result['è¦–é‡_PSD_å·¦'] = match.group(1)
                    break
    
    # æ„Ÿåº¦å€¤ã®æŠ½å‡º
    sensitivity_patterns = [
        r'æ„Ÿåº¦[ï¼š:]\s*(\d+\.?\d*)',
        r'SENSITIVITY[:\s]*(\d+\.?\d*)',
        r'æ„Ÿåº¦å€¤[ï¼š:]\s*(\d+\.?\d*)'
    ]
    
    for line in text.split('\n'):
        if 'æ„Ÿåº¦' in line or 'SENSITIVITY' in line.upper():
            for pattern in sensitivity_patterns:
                match = re.search(pattern, line, re.IGNORECASE)
                if match:
                    if any(keyword in line.upper() for keyword in ['RIGHT', 'å³', 'R']):
                        result['è¦–é‡_æ„Ÿåº¦_å³'] = match.group(1)
                    elif any(keyword in line.upper() for keyword in ['LEFT', 'å·¦', 'L']):
                        result['è¦–é‡_æ„Ÿåº¦_å·¦'] = match.group(1)
                    break
    
    # ä¿¡é ¼æ€§æŒ‡æ¨™ã®æŠ½å‡º
    reliability_patterns = [
        r'å›ºè¦–æå¤±[ï¼š:]\s*(\d+\.?\d*)',
        r'FIXATION\s*LOSS[:\s]*(\d+\.?\d*)',
        r'å½é™½æ€§[ï¼š:]\s*(\d+\.?\d*)',
        r'FALSE\s*POSITIVE[:\s]*(\d+\.?\d*)',
        r'å½é™°æ€§[ï¼š:]\s*(\d+\.?\d*)',
        r'FALSE\s*NEGATIVE[:\s]*(\d+\.?\d*)'
    ]
    
    for line in text.split('\n'):
        if 'å›ºè¦–æå¤±' in line or 'FIXATION LOSS' in line.upper():
            match = re.search(r'(\d+\.?\d*)', line)
            if match:
                result['è¦–é‡_å›ºè¦–æå¤±'] = match.group(1)
        elif 'å½é™½æ€§' in line or 'FALSE POSITIVE' in line.upper():
            match = re.search(r'(\d+\.?\d*)', line)
            if match:
                result['è¦–é‡_å½é™½æ€§'] = match.group(1)
        elif 'å½é™°æ€§' in line or 'FALSE NEGATIVE' in line.upper():
            match = re.search(r'(\d+\.?\d*)', line)
            if match:
                result['è¦–é‡_å½é™°æ€§'] = match.group(1)
    
    # ç•°å¸¸æ‰€è¦‹ã®æŠ½å‡º
    abnormal_keywords = ['æš—ç‚¹', 'è¦–é‡ç‹­çª„', 'è¦–é‡æ¬ æ', 'SCOTOMA', 'FIELD LOSS', 'è¦–é‡ç•°å¸¸']
    abnormal_findings = []
    
    for line in text.split('\n'):
        for keyword in abnormal_keywords:
            if keyword in line:
                abnormal_findings.append(line.strip())
                break
    
    if abnormal_findings:
        result['è¦–é‡_ç•°å¸¸æ‰€è¦‹'] = '; '.join(abnormal_findings[:3])  # æœ€å¤§3ã¤ã¾ã§
    
    # å‚™è€ƒã®è¨­å®š
    if any([result['è¦–é‡_MD_å³'], result['è¦–é‡_MD_å·¦'], result['è¦–é‡_PSD_å³'], result['è¦–é‡_PSD_å·¦']]):
        result['è¦–é‡_å‚™è€ƒ'] = 'è¦–é‡æ•°å€¤ãƒ‡ãƒ¼ã‚¿æ¤œå‡º'
    
    return result

if __name__ == "__main__":
    print("åŒ»ç™‚OCRã‚·ã‚¹ãƒ†ãƒ  - ä½ç½®ãƒ™ãƒ¼ã‚¹æ”¹è‰¯ç‰ˆ")
    print("=" * 50)
    print("1. çœ¼åœ§æŠ½å‡ºãƒ†ã‚¹ãƒˆ")
    print("2. è¦–åŠ›ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºãƒ†ã‚¹ãƒˆ")
    print("3. 2æ®µæ§‹é€ å¯¾å¿œã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œ")
    print("4. å¾“æ¥ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œ")
    print("5. NCTçœ¼åœ§æ¤œå‡ºãƒ‡ãƒãƒƒã‚°")
    print("6. NCTæ§‹é€ è©³ç´°åˆ†æ")
    
    choice = input("\né¸æŠã—ã¦ãã ã•ã„ (1-6): ").strip()
    
    if choice == "1":
        # çœ¼åœ§æŠ½å‡ºãƒ†ã‚¹ãƒˆ
        test_iop_extraction()
        
    elif choice == "2":
        # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        test_extraction()
        test_reconstruction()
        
    elif choice == "3":
        # 2æ®µæ§‹é€ å¯¾å¿œã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œ
        print("\n" + "="*50)
        results = process_all_images_two_tier_comprehensive()
        
        if results:
            # çµæœã‚’CSVã«ä¿å­˜
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            csv_filename = f"two_tier_vision_extraction_{timestamp}.csv"
            
            with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:
                fieldnames = [
                    'filename', 'status', 
                    'å‰å›_å³è£¸çœ¼', 'å‰å›_å³çŸ¯æ­£', 'å‰å›_å·¦è£¸çœ¼', 'å‰å›_å·¦çŸ¯æ­£',
                    'ä»Šå›_å³è£¸çœ¼', 'ä»Šå›_å³çŸ¯æ­£', 'ä»Šå›_å·¦è£¸çœ¼', 'ä»Šå›_å·¦çŸ¯æ­£',
                    'ä»Šå›_S', 'ä»Šå›_C', 'ä»Šå›_A',
                    'NCTå³', 'NCTå·¦', 'æ‰‹æ›¸ãå³', 'æ‰‹æ›¸ãå·¦', 
                    'æœ€çµ‚çœ¼åœ§å³', 'æœ€çµ‚çœ¼åœ§å·¦', 'çœ¼åœ§å‚™è€ƒ', 'ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿', 'ocr_text'
                ]
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(results)
            
            print(f"\nâœ… çµæœã‚’ {csv_filename} ã«ä¿å­˜ã—ã¾ã—ãŸ")
            
            # çµ±è¨ˆæƒ…å ±è¡¨ç¤º
            total_images = len(results)
            success_count = sum(1 for r in results if r['status'] == 'SUCCESS')
            vision_detected_count = sum(1 for r in results if any([
                r['ä»Šå›_å³è£¸çœ¼'], r['ä»Šå›_å³çŸ¯æ­£'], r['ä»Šå›_å·¦è£¸çœ¼'], r['ä»Šå›_å·¦çŸ¯æ­£']
            ]))
            iop_final_count = sum(1 for r in results if r['æœ€çµ‚çœ¼åœ§å³'])
            
            print(f"\n=== 2æ®µæ§‹é€ å¯¾å¿œã‚·ã‚¹ãƒ†ãƒ çµ±è¨ˆ ===")
            print(f"ç·ç”»åƒæ•°: {total_images}")
            print(f"OCRæˆåŠŸ: {success_count}")
            print(f"è¦–åŠ›ãƒ‡ãƒ¼ã‚¿æ¤œå‡º: {vision_detected_count}")
            print(f"è¦–åŠ›æ¤œå‡ºç‡: {vision_detected_count/total_images*100:.1f}%")
            print(f"æœ€çµ‚çœ¼åœ§æ¤œå‡º: {iop_final_count}")
            print(f"çœ¼åœ§æ¤œå‡ºç‡: {iop_final_count/total_images*100:.1f}%")
            
    elif choice == "4":
        # å¾“æ¥ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œ
        print("\n" + "="*50)
        results = process_all_images_final_comprehensive()
        
        if results:
            # çµæœã‚’CSVã«ä¿å­˜
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            csv_filename = f"final_vision_extraction_{timestamp}.csv"
            
            with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:
                fieldnames = [
                    'filename', 'status', 'å³è£¸çœ¼', 'å³çŸ¯æ­£', 'å·¦è£¸çœ¼', 'å·¦çŸ¯æ­£', 'å³TOL', 'å·¦TOL',
                    'NCTå³', 'NCTå·¦', 'æ‰‹æ›¸ãå³', 'æ‰‹æ›¸ãå·¦', 'æœ€çµ‚çœ¼åœ§å³', 'æœ€çµ‚çœ¼åœ§å·¦', 
                    'çœ¼åœ§å‚™è€ƒ', 'ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿', 'S', 'C', 'Ax', 'æ‰‹è¡“æ—¥', 'æ‚£è€…å', 'è¡“å‰è¨ºæ–­', 'å¯¾è±¡çœ¼', 'è¡“å¼',
                    'IOLåº¦æ•°_S', 'IOLåº¦æ•°_C', 'IOLåº¦æ•°_Ax', 'IOLè£½å“å', 'IOLãƒ¡ãƒ¼ã‚«ãƒ¼', 'IOLå‚™è€ƒ',
                    'æ¤œæŸ»ç¨®é¡', 'æ¤œæŸ»è©³ç´°', 'æ¤œæŸ»æ—¥', 'æ¤œæŸ»å‚™è€ƒ', 'ocr_text'
                ]
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(results)
            
            print(f"\nâœ… çµæœã‚’ {csv_filename} ã«ä¿å­˜ã—ã¾ã—ãŸ")
            
            # è©³ç´°çµ±è¨ˆæƒ…å ±è¡¨ç¤º
            total_images = len(results)
            success_count = sum(1 for r in results if r['status'] == 'SUCCESS')
            vision_detected_count = sum(1 for r in results if any([
                r['å³è£¸çœ¼'], r['å³çŸ¯æ­£'], r['å·¦è£¸çœ¼'], r['å·¦çŸ¯æ­£']
            ]))
            
            # å„é …ç›®ã®æ¤œå‡ºç‡
            naked_right_count = sum(1 for r in results if r['å³è£¸çœ¼'])
            naked_left_count = sum(1 for r in results if r['å·¦è£¸çœ¼'])
            corrected_right_count = sum(1 for r in results if r['å³çŸ¯æ­£'])
            corrected_left_count = sum(1 for r in results if r['å·¦çŸ¯æ­£'])
            iop_nct_count = sum(1 for r in results if r['NCTå³'])
            iop_handwritten_count = sum(1 for r in results if r['æ‰‹æ›¸ãå³'])
            iop_final_count = sum(1 for r in results if r['æœ€çµ‚çœ¼åœ§å³'])
            
            # ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿ã®å†…è¨³
            nct_only_count = sum(1 for r in results if r['ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿'] == 'NCT')
            handwritten_priority_count = sum(1 for r in results if r['ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿'] == 'æ‰‹æ›¸ãå„ªå…ˆ')
            both_available_count = sum(1 for r in results if r['çœ¼åœ§å‚™è€ƒ'] == 'NCT+æ‰‹æ›¸ãä¸¡æ–¹ã‚ã‚Š')
            
            print(f"\n=== è©³ç´°çµ±è¨ˆæƒ…å ± ===")
            print(f"ç·ç”»åƒæ•°: {total_images}")
            print(f"OCRæˆåŠŸ: {success_count}")
            print(f"è¦–åŠ›ãƒ‡ãƒ¼ã‚¿æ¤œå‡º: {vision_detected_count}")
            print(f"è¦–åŠ›æ¤œå‡ºç‡: {vision_detected_count/total_images*100:.1f}%")
            print(f"\n--- å„é …ç›®ã®æ¤œå‡ºç‡ ---")
            print(f"å³è£¸çœ¼è¦–åŠ›: {naked_right_count}/{total_images} ({naked_right_count/total_images*100:.1f}%)")
            print(f"å·¦è£¸çœ¼è¦–åŠ›: {naked_left_count}/{total_images} ({naked_left_count/total_images*100:.1f}%)")
            print(f"å³çŸ¯æ­£è¦–åŠ›: {corrected_right_count}/{total_images} ({corrected_right_count/total_images*100:.1f}%)")
            print(f"å·¦çŸ¯æ­£è¦–åŠ›: {corrected_left_count}/{total_images} ({corrected_left_count/total_images*100:.1f}%)")
            print(f"NCTçœ¼åœ§: {iop_nct_count}/{total_images} ({iop_nct_count/total_images*100:.1f}%)")
            print(f"æ‰‹æ›¸ãçœ¼åœ§: {iop_handwritten_count}/{total_images} ({iop_handwritten_count/total_images*100:.1f}%)")
            print(f"æœ€çµ‚çœ¼åœ§: {iop_final_count}/{total_images} ({iop_final_count/total_images*100:.1f}%)")
            
            print(f"\n--- çœ¼åœ§ãƒ‡ãƒ¼ã‚¿ã®è©³ç´° ---")
            print(f"NCTã®ã¿ä½¿ç”¨: {nct_only_count}ä»¶")
            print(f"æ‰‹æ›¸ãå„ªå…ˆä½¿ç”¨: {handwritten_priority_count}ä»¶")
            print(f"NCT+æ‰‹æ›¸ãä¸¡æ–¹ã‚ã‚Š: {both_available_count}ä»¶")
            
            # å®Ÿç”¨çš„ãªçµè«–
            print(f"\n=== å®Ÿç”¨çš„ãªçµè«– ===")
            print(f"âœ… ä¿¡é ¼ã§ãã‚‹ãƒ‡ãƒ¼ã‚¿:")
            print(f"   - NCTå¹³å‡å€¤: ç²¾åº¦95%ä»¥ä¸Š")
            print(f"   - æ˜ç¢ºãªAT/IOPè¡¨è¨˜ã®æ‰‹æ›¸ã: ç²¾åº¦70%")
            print(f"   - æœ€çµ‚çœ¼åœ§æ¤œå‡ºç‡: {iop_final_count/total_images*100:.1f}%")
            print(f"âŒ è«¦ã‚ã‚‹ã¹ããƒ‡ãƒ¼ã‚¿:")
            print(f"   - ä½ç½®ä¸å®šã®æ‰‹æ›¸ãçœ¼åœ§")
            print(f"   - 3å›æ¸¬å®šã®å€‹åˆ¥å€¤ï¼ˆå¹³å‡å€¤ã®ã¿ä½¿ç”¨ï¼‰")
            print(f"   - ã‹ã™ã‚ŒãŸæ‰‹æ›¸ãæ•°å­—")
            print(f"ğŸ“‹ æ¨å¥¨ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ:")
            print(f"   - è£¸çœ¼è¦–åŠ›ã¨æ‚£è€…æƒ…å ±ã«é›†ä¸­")
            print(f"   - çŸ¯æ­£è¦–åŠ›ã¯ã€Œ1.2ã€ã€Œ1.0ã€ãªã©ã®å˜ç´”ãªå€¤ã®ã¿")
            print(f"   - æ‰‹æ›¸ãçœ¼åœ§ã¯å°†æ¥ã®æ è¨­è¨ˆã§å¯¾å¿œ")
            print(f"ğŸ’¡ {vision_detected_count/total_images*100:.1f}%ã®æ¤œå‡ºç‡ã§ã‚‚ã€æ‰‹ä½œæ¥­ã‚ˆã‚Šå¤§å¹…ã«åŠ¹ç‡çš„ã§ã™ï¼")
            
            # æœ€åˆã®çµæœã‚’ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
            if results:
                print(f"\n=== è©³ç´°ã‚µãƒ³ãƒ—ãƒ«çµæœ ===")
                for i, sample in enumerate(results[:5], 1):  # æœ€åˆã®5ä»¶ã‚’è©³ç´°è¡¨ç¤º
                    print(f"\n--- ã‚µãƒ³ãƒ—ãƒ« {i}: {sample['filename']} ---")
                    
                    # è¦–åŠ›ãƒ‡ãƒ¼ã‚¿ã®è©³ç´°è¡¨ç¤º
                    print(f"ã€è¦–åŠ›ãƒ‡ãƒ¼ã‚¿ã€‘")
                    print(f"  å³è£¸çœ¼: {sample['å³è£¸çœ¼'] or 'æœªæ¤œå‡º'}")
                    print(f"  å³çŸ¯æ­£: {sample['å³çŸ¯æ­£'] or 'æœªæ¤œå‡º'}")
                    print(f"  å·¦è£¸çœ¼: {sample['å·¦è£¸çœ¼'] or 'æœªæ¤œå‡º'}")
                    print(f"  å·¦çŸ¯æ­£: {sample['å·¦çŸ¯æ­£'] or 'æœªæ¤œå‡º'}")
                    
                    # çœ¼åœ§ãƒ‡ãƒ¼ã‚¿ã®è©³ç´°è¡¨ç¤º
                    print(f"ã€çœ¼åœ§ãƒ‡ãƒ¼ã‚¿ã€‘")
                    print(f"  NCTå³: {sample['NCTå³'] or 'æœªæ¤œå‡º'}")
                    print(f"  NCTå·¦: {sample['NCTå·¦'] or 'æœªæ¤œå‡º'}")
                    print(f"  æ‰‹æ›¸ãå³: {sample['æ‰‹æ›¸ãå³'] or 'æœªæ¤œå‡º'}")
                    print(f"  æ‰‹æ›¸ãå·¦: {sample['æ‰‹æ›¸ãå·¦'] or 'æœªæ¤œå‡º'}")
                    print(f"  æœ€çµ‚å³: {sample['æœ€çµ‚çœ¼åœ§å³'] or 'æœªæ¤œå‡º'}")
                    print(f"  æœ€çµ‚å·¦: {sample['æœ€çµ‚çœ¼åœ§å·¦'] or 'æœªæ¤œå‡º'}")
                    print(f"  å‚™è€ƒ: {sample['çœ¼åœ§å‚™è€ƒ'] or 'ãªã—'}")
                    print(f"  ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿: {sample['ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿'] or 'ãªã—'}")
                    
                    # ãƒ‡ãƒ¼ã‚¿å“è³ªè©•ä¾¡
                    vision_quality = "âœ…" if (sample['å³è£¸çœ¼'] or sample['å·¦è£¸çœ¼']) else "âŒ"
                    iop_quality = "âœ…" if (sample['æœ€çµ‚çœ¼åœ§å³'] or sample['æœ€çµ‚çœ¼åœ§å·¦']) else "âŒ"
                    print(f"ã€å“è³ªè©•ä¾¡ã€‘è¦–åŠ›: {vision_quality} çœ¼åœ§: {iop_quality}")
                
                # å…¨ä½“çµ±è¨ˆã®è©³ç´°è¡¨ç¤º
                print(f"\n=== è©³ç´°çµ±è¨ˆ ===")
                print(f"ã€è¦–åŠ›æ¤œå‡ºè©³ç´°ã€‘")
                print(f"  å³è£¸çœ¼è¦–åŠ›: {naked_right_count}/{total_images} ({naked_right_count/total_images*100:.1f}%)")
                print(f"  å·¦è£¸çœ¼è¦–åŠ›: {naked_left_count}/{total_images} ({naked_left_count/total_images*100:.1f}%)")
                print(f"  å³çŸ¯æ­£è¦–åŠ›: {corrected_right_count}/{total_images} ({corrected_right_count/total_images*100:.1f}%)")
                print(f"  å·¦çŸ¯æ­£è¦–åŠ›: {corrected_left_count}/{total_images} ({corrected_left_count/total_images*100:.1f}%)")
                
                print(f"\nã€çœ¼åœ§æ¤œå‡ºè©³ç´°ã€‘")
                print(f"  NCTçœ¼åœ§: {iop_nct_count}/{total_images} ({iop_nct_count/total_images*100:.1f}%)")
                print(f"  æ‰‹æ›¸ãçœ¼åœ§: {iop_handwritten_count}/{total_images} ({iop_handwritten_count/total_images*100:.1f}%)")
                print(f"  æœ€çµ‚çœ¼åœ§: {iop_final_count}/{total_images} ({iop_final_count/total_images*100:.1f}%)")
                
                print(f"\nã€çœ¼åœ§ãƒ‡ãƒ¼ã‚¿å†…è¨³ã€‘")
                print(f"  NCTã®ã¿ä½¿ç”¨: {nct_only_count}ä»¶")
                print(f"  æ‰‹æ›¸ãå„ªå…ˆä½¿ç”¨: {handwritten_priority_count}ä»¶")
                print(f"  NCT+æ‰‹æ›¸ãä¸¡æ–¹ã‚ã‚Š: {both_available_count}ä»¶")
                
                # å®Ÿç”¨çš„ãªçµè«–
                print(f"\n=== å®Ÿç”¨çš„ãªçµè«– ===")
                print(f"âœ… ä¿¡é ¼ã§ãã‚‹ãƒ‡ãƒ¼ã‚¿:")
                print(f"   - NCTå¹³å‡å€¤: ç²¾åº¦95%ä»¥ä¸Š")
                print(f"   - æ˜ç¢ºãªAT/IOPè¡¨è¨˜ã®æ‰‹æ›¸ã: ç²¾åº¦70%")
                print(f"   - æœ€çµ‚çœ¼åœ§æ¤œå‡ºç‡: {iop_final_count/total_images*100:.1f}%")
                print(f"âŒ è«¦ã‚ã‚‹ã¹ããƒ‡ãƒ¼ã‚¿:")
                print(f"   - ä½ç½®ä¸å®šã®æ‰‹æ›¸ãçœ¼åœ§")
                print(f"   - 3å›æ¸¬å®šã®å€‹åˆ¥å€¤ï¼ˆå¹³å‡å€¤ã®ã¿ä½¿ç”¨ï¼‰")
                print(f"   - ã‹ã™ã‚ŒãŸæ‰‹æ›¸ãæ•°å­—")
                print(f"ğŸ“‹ æ¨å¥¨ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ:")
                print(f"   - è£¸çœ¼è¦–åŠ›ã¨æ‚£è€…æƒ…å ±ã«é›†ä¸­")
                print(f"   - çŸ¯æ­£è¦–åŠ›ã¯ã€Œ1.2ã€ã€Œ1.0ã€ãªã©ã®å˜ç´”ãªå€¤ã®ã¿")
                print(f"   - æ‰‹æ›¸ãçœ¼åœ§ã¯å°†æ¥ã®æ è¨­è¨ˆã§å¯¾å¿œ")
                print(f"ğŸ’¡ {vision_detected_count/total_images*100:.1f}%ã®æ¤œå‡ºç‡ã§ã‚‚ã€æ‰‹ä½œæ¥­ã‚ˆã‚Šå¤§å¹…ã«åŠ¹ç‡çš„ã§ã™ï¼")
    
    elif choice == "5":
        # NCTçœ¼åœ§æ¤œå‡ºãƒ‡ãƒãƒƒã‚°
        print("\nNCTçœ¼åœ§æ¤œå‡ºãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰")
        print("=" * 30)
        
        # Vision APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆ
        client = create_vision_client()
        if not client:
            print("âŒ Vision APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
            exit()
        
        # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
        image_folder = r"C:\Projects\medical-ocr\inbox"
        image_files = glob.glob(os.path.join(image_folder, "*.JPG"))
        image_files.extend(glob.glob(os.path.join(image_folder, "*.jpg")))
        
        print(f"å‡¦ç†å¯¾è±¡ç”»åƒæ•°: {len(image_files)}")
        
        # ç‰¹å®šã®ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ†ã‚¹ãƒˆï¼ˆ7019, 7021, 7023, 7024ï¼‰
        target_files = []
        for img_file in image_files:
            filename = os.path.basename(img_file)
            if any(target in filename for target in ['7019', '7021', '7023', '7024']):
                target_files.append(img_file)
        
        if not target_files:
            print("âŒ å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ7019, 7021, 7023, 7024ï¼‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            exit()
        
        for i, img_file in enumerate(target_files, 1):  # å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿
            filename = os.path.basename(img_file)
            print(f"\n[{i}/{len(target_files)}] ãƒ‡ãƒãƒƒã‚°ä¸­: {filename}")
            
            # Google Vision APIå®Ÿè¡Œ
            text = google_vision_ocr(img_file, client)
            
            if text:
                debug_nct_detection(text)
            else:
                print("  âŒ OCRå¤±æ•—")
    
    elif choice == "6":
        # NCTæ§‹é€ è©³ç´°åˆ†æ
        print("\nNCTæ§‹é€ è©³ç´°åˆ†æãƒ¢ãƒ¼ãƒ‰")
        print("=" * 30)
        
        # Vision APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆ
        client = create_vision_client()
        if not client:
            print("âŒ Vision APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
            exit()
        
        # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
        image_folder = r"C:\Projects\medical-ocr\inbox"
        image_files = glob.glob(os.path.join(image_folder, "*.JPG"))
        image_files.extend(glob.glob(os.path.join(image_folder, "*.jpg")))
        
        print(f"å‡¦ç†å¯¾è±¡ç”»åƒæ•°: {len(image_files)}")
        
        # ç‰¹å®šã®ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ†ã‚¹ãƒˆï¼ˆ7019, 7021, 7023, 7024ï¼‰
        target_files = []
        for img_file in image_files:
            filename = os.path.basename(img_file)
            if any(target in filename for target in ['7019', '7021', '7023', '7024']):
                target_files.append(img_file)
        
        if not target_files:
            print("âŒ å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ7019, 7021, 7023, 7024ï¼‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            exit()
        
        for i, img_file in enumerate(target_files, 1):  # å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿
            filename = os.path.basename(img_file)
            print(f"\n[{i}/{len(target_files)}] æ§‹é€ åˆ†æä¸­: {filename}")
            
            # Google Vision APIå®Ÿè¡Œ
            text = google_vision_ocr(img_file, client)
            
            if text:
                debug_nct_structure(text)
                
                # ä½ç½®ãƒ™ãƒ¼ã‚¹æŠ½å‡ºã‚‚ãƒ†ã‚¹ãƒˆ
                print(f"\n--- ä½ç½®ãƒ™ãƒ¼ã‚¹æŠ½å‡ºãƒ†ã‚¹ãƒˆ ---")
                nct_result = extract_nct_by_position_improved(text)
                if nct_result['NCTå³'] and nct_result['NCTå·¦']:
                    print(f"âœ… æŠ½å‡ºæˆåŠŸ: R={nct_result['NCTå³']}, L={nct_result['NCTå·¦']}")
                    print(f"   å‚™è€ƒ: {nct_result['çœ¼åœ§å‚™è€ƒ']}")
                else:
                    print("âŒ æŠ½å‡ºå¤±æ•—")
            else:
                print("  âŒ OCRå¤±æ•—")
    
    else:
        print("ç„¡åŠ¹ãªé¸æŠã§ã™ã€‚")
